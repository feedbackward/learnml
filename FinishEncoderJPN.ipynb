{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# エンコーダを学習する\n",
    "\n",
    "__目次：__\n",
    "\n",
    "- <a href=\"#raw2feat\">視覚刺激から特徴量を造る</a>\n",
    "- <a href=\"#algorun\">モデルを初期化してアルゴリズムを走らせる</a>\n",
    "- <a href=\"#eval\">学習機の出来を評価する</a>\n",
    "- <a href=\"#tasks\">課題一覧</a>\n",
    "\n",
    "___\n",
    "\n",
    "ここでの目的は、これまでに習得してきた技法や知見を融合させ、正常に機能する学習機を造ることである。その応用先は視覚刺激から脳活動へのエンコーディングである。まずはこれまでに用意してきた関数やクラスを`import`しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import scripts.FilterBank as fb\n",
    "import scripts.AlgoIntro as ai\n",
    "import scripts.AlgoSparseReg as asr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作業の流れは下記の通りになる：\n",
    "\n",
    "    視覚刺激の読み込み --> ガボールフィルタで特徴量を作って保存 --> スパースな線形回帰\n",
    "\n",
    "これらのタスクを一つずつこなしていく。やり方を明確に伝えるべく、非常に単純なプロトタイプを作っておく。その後の課題では、このプロトタイプをベースにして、エンコーダとしてちゃんと機能するように改善してもらうことになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"raw2feat\"></a>\n",
    "## 視覚刺激から特徴量を造る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Import the vim-2 data from the Python binary format we saved earlier.\n",
    "# Assumptions: that the data is saved in the vim-2 directory,\n",
    "# already of 96x96 size, with dtype of np.float32.\n",
    "\n",
    "PIX_W = 96\n",
    "PIX_H = 96\n",
    "dtype=np.float32\n",
    "\n",
    "# Read the raw training data.\n",
    "shape=(PIX_W,PIX_H,3,108000) # (downsized px, downsized px, rgd channels, time steps)\n",
    "# Index for temporal down-sampling. Alternatives: do after the feature-building and aggregate.\n",
    "idx_ds = np.arange(15//2, 108000+1, 15) # need length 7200.\n",
    "fname = \"data/vim-2/X_tr.dat\"\n",
    "with open(fname, mode=\"br\") as fbin:\n",
    "    print(\"Reading...\", end=\" \")\n",
    "    raw_tr = np.fromfile(file=fbin,dtype=dtype).reshape(shape)[:,:,:,idx_ds] # temporally down-sampled.\n",
    "    print(\"OK.\")\n",
    "\n",
    "# Check a few frames.\n",
    "#num_frames = raw_tr.shape[3]\n",
    "#frames_to_play = 5\n",
    "#for t in range(frames_to_play):\n",
    "#    plt.imshow(raw_tr[:,:,:,t])\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "# Read the raw testing data.\n",
    "shape=(PIX_W,PIX_H,3,8100) # (downsized px, downsized px, rgd channels, time steps)\n",
    "# Index for temporal down-sampling. Alternatives: do after the feature-building and aggregate.\n",
    "idx_ds = np.arange(15//2, 8100+1, 15) # need length 540.\n",
    "fname = \"data/vim-2/X_te.dat\"\n",
    "with open(fname, mode=\"br\") as fbin:\n",
    "    print(\"Reading...\", end=\" \")\n",
    "    raw_te = np.fromfile(file=fbin, dtype=dtype).reshape(shape)[:,:,:,idx_ds] # temporally down-sampled.\n",
    "    print(\"OK.\")\n",
    "\n",
    "# Check a few frames.\n",
    "#num_frames = raw_tr.shape[3]\n",
    "#frames_to_play = 5\n",
    "#for t in range(frames_to_play):\n",
    "#    plt.imshow(raw_tr[:,:,:,t])\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up the parameters that specify the first filter bank.\n",
    "myparas = {\"freqs\": 32/max(PIX_W,PIX_H),\n",
    "           \"dir\": math.pi/2,\n",
    "           \"amp\": 0.1,\n",
    "           \"sdev\": max(PIX_W,PIX_H)/20,\n",
    "           \"phase\": 0}\n",
    "mygrid_h = 3\n",
    "mygrid_w = 3\n",
    "\n",
    "print(\"Getting features (tr)...\", end=\" \")\n",
    "X_tr = fb.G2_getfeatures(ims=raw_tr,\n",
    "                         fil_paras=myparas,\n",
    "                         gridshape=(mygrid_h,mygrid_w),\n",
    "                         mode=\"reflect\", cval=0)\n",
    "print(\"OK.\")\n",
    "\n",
    "print(\"Getting features (te)...\", end=\" \")\n",
    "X_te = fb.G2_getfeatures(ims=raw_te,\n",
    "                         fil_paras=myparas,\n",
    "                         gridshape=(mygrid_h,mygrid_w),\n",
    "                         mode=\"reflect\", cval=0)\n",
    "print(\"OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the produced features:\")\n",
    "print(\"tr:\", X_tr.shape)\n",
    "print(\"te:\", X_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のパラメータだけでは、特徴量が非常に少なく、説明能力が到底足りない。増やす方法はいくらでもあるが、別のdictを一つ用意し、特徴量ベクトルを連結させる事例を次に示しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters that specify the second filter bank.\n",
    "myparas = {\"freqs\": 32/max(PIX_W,PIX_H),\n",
    "           \"dir\": 0,\n",
    "           \"amp\": 0.1,\n",
    "           \"sdev\": max(PIX_W,PIX_H)/20,\n",
    "           \"phase\": 0}\n",
    "mygrid_h = 9\n",
    "mygrid_w = 9\n",
    "\n",
    "print(\"Getting features (tr)...\", end=\" \")\n",
    "tmp_X = fb.G2_getfeatures(ims=raw_tr,\n",
    "                         fil_paras=myparas,\n",
    "                         gridshape=(mygrid_h,mygrid_w),\n",
    "                         mode=\"reflect\", cval=0)\n",
    "print(\"OK.\")\n",
    "X_tr = np.concatenate((X_tr, tmp_X), axis=1) # concatenate!\n",
    "\n",
    "print(\"Getting features (te)...\", end=\" \")\n",
    "tmp_X = fb.G2_getfeatures(ims=raw_te,\n",
    "                         fil_paras=myparas,\n",
    "                         gridshape=(mygrid_h,mygrid_w),\n",
    "                         mode=\"reflect\", cval=0)\n",
    "print(\"OK.\")\n",
    "X_te = np.concatenate((X_te, tmp_X), axis=1) # concatenate!\n",
    "\n",
    "print(\"Shape of the produced features:\")\n",
    "print(\"tr:\", X_tr.shape)\n",
    "print(\"te:\", X_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上から明らかなように、連結させることで、データ行列に列を追加しているだけである。行数（つまりサンプル数）は一定である。\n",
    "\n",
    "これまでと同様に、Nishimoto et al. (2011)にしたがって、特徴量のZスコアを計算する（平均ゼロ、分散1.0の標準化）。これをした上で、「外れ値」と見なすべき点の閾値を定め、必要に応じて切断する。公平な学習課題になるように、訓練・検証を別々に扱う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_tr = X_tr - np.mean(X_tr, axis=0)\n",
    "X_tr = X_tr / np.std(X_tr, axis=0)\n",
    "print(\"Mean =\", np.mean(X_tr, axis=0), \"StdDev =\", np.std(X_tr, axis=0))\n",
    "\n",
    "X_te = X_te - np.mean(X_te, axis=0)\n",
    "X_te = X_te / np.std(X_te, axis=0)\n",
    "print(\"Mean =\", np.mean(X_te, axis=0), \"StdDev =\", np.std(X_te, axis=0))\n",
    "\n",
    "for j in range(X_tr.shape[1]):\n",
    "    stdval = np.std(X_tr[:,j])\n",
    "    X_tr[:,j] = np.clip(X_tr[:,j], a_min=(-stdval), a_max=stdval)\n",
    "    stdval = np.std(X_te[:,j])\n",
    "    X_te[:,j] = np.clip(X_te[:,j], a_min=(-stdval), a_max=stdval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思い通りにこれまでの計算ができているのであれば、整列された全特徴量をディスクに書き込み、付属data infoオブジェクトも併せて用意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import support.classes as classes\n",
    "\n",
    "# Make a data_info file for this data, and save as data/encoder/info.dat.\n",
    "dinfo = classes.DataInfo()\n",
    "dinfo.mname = \"Encoder\"\n",
    "\n",
    "fname = \"data/encoder/X_tr.dat\"\n",
    "dtype = raw_tr.dtype\n",
    "shape = raw_tr.shape\n",
    "with open(fname, mode=\"bw\") as fbin:\n",
    "    X_tr.tofile(fbin)\n",
    "    print(\"Saved to file.\")\n",
    "dinfo.X_tr[\"shape\"] = X_tr.shape\n",
    "dinfo.X_tr[\"path\"] = \"data/encoder/X_tr.dat\"\n",
    "dinfo.X_tr[\"dtype\"] = X_tr.dtype\n",
    "    \n",
    "fname = \"data/encoder/X_te.dat\"\n",
    "dtype = raw_te.dtype\n",
    "shape = raw_te.shape\n",
    "with open(fname, mode=\"bw\") as fbin:\n",
    "    X_te.tofile(fbin)\n",
    "    print(\"Saved to file.\")\n",
    "dinfo.X_te[\"shape\"] = X_te.shape\n",
    "dinfo.X_te[\"path\"] = \"data/encoder/X_te.dat\"\n",
    "dinfo.X_te[\"dtype\"] = X_te.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import support.classes as classes\n",
    "\n",
    "# Make a data_info file for this data, and save as data/encoder/info.dat.\n",
    "dinfo = classes.DataInfo()\n",
    "dinfo.mname = \"Encoder\"\n",
    "\n",
    "fname = \"data/encoder/X_tr.dat\"\n",
    "dtype = raw_tr.dtype\n",
    "shape = raw_tr.shape\n",
    "with open(fname, mode=\"bw\") as fbin:\n",
    "    X_tr.tofile(fbin)\n",
    "    print(\"Saved to file.\")\n",
    "dinfo.X_tr[\"shape\"] = X_tr.shape\n",
    "dinfo.X_tr[\"path\"] = \"data/encoder/X_tr.dat\"\n",
    "dinfo.X_tr[\"dtype\"] = X_tr.dtype\n",
    "\n",
    "fname = \"data/encoder/X_te.dat\"\n",
    "dtype = raw_te.dtype\n",
    "shape = raw_te.shape\n",
    "with open(fname, mode=\"bw\") as fbin:\n",
    "    X_te.tofile(fbin)\n",
    "    print(\"Saved to file.\")\n",
    "dinfo.X_te[\"shape\"] = X_te.shape\n",
    "dinfo.X_te[\"path\"] = \"data/encoder/X_te.dat\"\n",
    "dinfo.X_te[\"dtype\"] = X_te.dtype\n",
    "\n",
    "# Clear the raw data from memory.\n",
    "del [raw_te, raw_tr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "応答に関しては、特別な処理は何も要らないので、`encoder`ディレクトリへ移すだけで十分である。\n",
    "\n",
    "```\n",
    "$ mv data/vim-2/y_tr.dat data/vim-2/y_te.dat ./data/encoder/\n",
    "$ mv data/vim-2/cleanidx_tr.dat data/vim-2/cleanidx_te.dat ./data/encoder/\n",
    "\n",
    "```\n",
    "\n",
    "`dinfo`の残りの空欄を埋めてからディスクに書き込む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "fname = \"data/encoder/y_tr.dat\"\n",
    "dinfo.y_tr[\"shape\"] = (73728, 7200)\n",
    "dinfo.y_tr[\"path\"] = \"data/encoder/y_tr.dat\"\n",
    "dinfo.y_tr[\"dtype\"] = np.float32\n",
    "\n",
    "fname = \"data/encoder/y_te.dat\"\n",
    "dinfo.y_te[\"shape\"] = (73728, 540)\n",
    "dinfo.y_te[\"path\"] = \"data/encoder/y_te.dat\"\n",
    "dinfo.y_te[\"dtype\"] = np.float32\n",
    "\n",
    "dinfo.misc = {\"voxidx\": None} # to be filled in later.\n",
    "\n",
    "with open(\"data/encoder/info.dat\", mode=\"bw\") as fbin:\n",
    "    pickle.dump(dinfo, fbin)\n",
    "    print(\"Saved to file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"algorun\"></a>\n",
    "## モデルを初期化してアルゴリズムを走らせる\n",
    "\n",
    "特徴量の準備が整っているなら、Jupyterのカーネルをリセットし、ここから新たな作業を始める。\n",
    "\n",
    "前回で作った`Algo_LASSO_CD`をここで本領発揮してもらう。前回のテストとまったく同様に、多数の$\\lambda$候補を用意し、warm startを生かしながら学習させていく。\n",
    "\n",
    "パフォーマンス評価指標として、Nishimoto et al. (2011)より：\n",
    "\n",
    "> *\"Prediction accuracy was defined as the correlation between predicted and observed BOLD signals. The averaged accuracy across subjects and voxels in early visual areas (V1, V2, V3, V3A, and V3B) was 0.24, 0.39, and 0.40 for the static, nondirectional, and directional encoding models, respectively.\"*\n",
    "\n",
    "教育目的なので我々の学習機があらゆる意味で彼らのものに劣っているのだが、めざす基準としてはこの数値には意義がある。特に相関係数0.24は、2次元のガボールフィルタを使ったときに出た数字なので、より現実的な基準と見てもよかろう。\n",
    "\n",
    "算出方法は次の通りである。訓練データを使って、学習アルゴリズムを一通り走らせると、$\\widehat{w}$に対応する`w_est`が定まる。線形モデルを使っているので、新しい入力$X$から$\\widehat{y} = X\\widehat{w}$を出して、$\\widehat{y} \\approx y$で近似してみる。この$X$と$y$が`X_te`と`y_te`に対応する。予測信号と本当の信号の相関が強いほどよいということなので、下記の通りに相関係数を出す：\n",
    "\n",
    "\\begin{align}\n",
    "\\text{corr}\\,(\\widehat{y},y) = \\frac{\\text{cov}\\,(\\widehat{y},y)}{\\sqrt{\\text{var}\\,(\\widehat{y})\\text{var}\\,(y)}},\n",
    "\\end{align}\n",
    "\n",
    "これは`scipy.stats.pearsonr`で計算できる。また、予測・真の平均的な2乗誤差（root mean squared error; RMSE）を求めることもできる：\n",
    "\n",
    "\\begin{align}\n",
    "\\text{RMSE}\\,(\\widehat{y},y) = \\left( \\frac{1}{m} \\sum_{i=1}^{m} (\\widehat{y}_{i}-y_{i})^2 \\right)^{1/2},\n",
    "\\end{align}\n",
    "\n",
    "これはモデルオブジェクトの`mod.eval`というメソッドで実装している。$m$は検証データのサンプル数（ここでは$m=540$）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import support.parse_model as mp\n",
    "import scripts.AlgoSparseReg as asr\n",
    "\n",
    "# Load the general data info object.\n",
    "with open(\"data/encoder/info.dat\", mode=\"br\") as fbin:\n",
    "    dinfo = pickle.load(fbin)\n",
    "\n",
    "# Load the clean index, extracting indices from binary indicators.\n",
    "with open(\"data/encoder/cleanidx_tr.dat\", mode=\"br\") as fbin:\n",
    "    #cleanidx_tr_RAW = np.fromfile(file=fbin, dtype=np.uint32)\n",
    "    cleanidx_tr = np.flatnonzero(np.fromfile(file=fbin, dtype=np.uint32))\n",
    "    print(\"length:\", cleanidx_tr.size)\n",
    "with open(\"data/encoder/cleanidx_te.dat\", mode=\"br\") as fbin:\n",
    "    #cleanidx_te_RAW = np.fromfile(file=fbin, dtype=np.uint32)\n",
    "    cleanidx_te = np.flatnonzero(np.fromfile(file=fbin, dtype=np.uint32))\n",
    "    print(\"length:\", cleanidx_te.size)\n",
    "    \n",
    "# Take the intersection of the clean voxel indices and sort.\n",
    "cleanidx = np.intersect1d(cleanidx_tr,cleanidx_te)\n",
    "print(cleanidx_tr)\n",
    "print(cleanidx_te)\n",
    "print(\"cleanidx length:\", cleanidx.size)\n",
    "\n",
    "# Load the data info object. We shall modify its voxel index on the fly.\n",
    "with open(\"data/encoder/info.dat\", mode=\"br\") as fbin:\n",
    "    dinfo = pickle.load(fbin)\n",
    "    print(dinfo)\n",
    "\n",
    "# Initialize model and weights for an individual voxel.\n",
    "dinfo.misc[\"voxidx\"] = cleanidx[0]\n",
    "mod = mp.model(dinfo)\n",
    "w_init = mod.w_initialize()\n",
    "\n",
    "\n",
    "lam_min = 1 / mod.n\n",
    "\n",
    "# TODO: set the START and STOP guys with proper computations using mod.X_tr and mod.y_tr.\n",
    "todo_lambda = np.flipud(np.logspace(start=math.log10(lam_min),\n",
    "                                    stop=math.log10(1),\n",
    "                                    num=150))\n",
    "\n",
    "# Store performance metric statistics for each lambda setting.\n",
    "err_overlam = np.zeros(todo_lambda.size, dtype=np.float32)\n",
    "corr_overlam = np.zeros(todo_lambda.size, dtype=np.float32)\n",
    "spar_overlam = np.zeros(todo_lambda.size, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over the lambda values once, for a single candidate.\n",
    "print(\"Working...\")\n",
    "for i in range(todo_lambda.size):\n",
    "    \n",
    "    # Initialize and execute the algorithm.\n",
    "    al = asr.Algo_LASSO_CD(w_init=w_init,\\\n",
    "                           t_max=20*w_init.size,\\\n",
    "                           lam_l1=todo_lambda[i],\\\n",
    "                           verbose=False)\n",
    "    \n",
    "    for mystep in al:\n",
    "        al.update(model=mod)\n",
    "    \n",
    "    # Record performance.\n",
    "    w_est = al.w\n",
    "    err_overlam[i] = mod.eval(w_est)[0]\n",
    "    if np.std(np.dot(mod.X_te, w_est)) > 0:\n",
    "        corrval = mod.corr_te(w_est)\n",
    "    else:\n",
    "        corrval = 0  # watch out for zero-variance case.\n",
    "    corr_overlam[i] = corrval\n",
    "    spar_overlam[i] = np.count_nonzero(w_est)\n",
    "    \n",
    "    # Update the initializer to the most current observation.\n",
    "    w_init = w_est\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習が終わると、その生の成績を`raw/encoder`というディレクトリに保存しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw/encoder/lasso01_lam.raw\", mode=\"bw\") as fbin:\n",
    "    pickle.dump(todo_lambda, fbin)\n",
    "    print(\"Saved to file.\")\n",
    "\n",
    "with open(\"raw/encoder/lasso01_err.raw\", mode=\"bw\") as fbin:\n",
    "    pickle.dump(err_overlam, fbin)\n",
    "    print(\"Saved to file.\")\n",
    "    \n",
    "with open(\"raw/encoder/lasso01_corr.raw\", mode=\"bw\") as fbin:\n",
    "    pickle.dump(corr_overlam, fbin)\n",
    "    print(\"Saved to file.\")\n",
    "    \n",
    "with open(\"raw/encoder/lasso01_spar.raw\", mode=\"bw\") as fbin:\n",
    "    pickle.dump(spar_overlam, fbin)\n",
    "    print(\"Saved to file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"eval\"></a>\n",
    "## 学習機の出来を評価する\n",
    "\n",
    "学習が終わって、あとは成績を集計したり、可視化したりするだけである。\n",
    "Jupyterのカーネルをリセットし、ここから新たな作業を始める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"raw/encoder/lasso01_lam.raw\", mode=\"br\") as fbin:\n",
    "    todo_lambda = pickle.load(fbin)\n",
    "\n",
    "with open(\"raw/encoder/lasso01_err.raw\", mode=\"br\") as fbin:\n",
    "    err_overlam = pickle.load(fbin)\n",
    "    \n",
    "with open(\"raw/encoder/lasso01_corr.raw\", mode=\"br\") as fbin:\n",
    "    corr_overlam = pickle.load(fbin)\n",
    "    \n",
    "with open(\"raw/encoder/lasso01_spar.raw\", mode=\"br\") as fbin:\n",
    "    spar_overlam = pickle.load(fbin)\n",
    "\n",
    "myfig = plt.figure(figsize=(18,4))\n",
    "ax_err = myfig.add_subplot(1, 3, 1)\n",
    "plt.ylabel(\"Root mean squared error (RMSE)\")\n",
    "plt.xlabel(\"Lambda values\")\n",
    "ax_err.set_yscale('log')\n",
    "ax_err.set_xscale('log')\n",
    "ax_err.plot(todo_lambda, err_overlam)\n",
    "\n",
    "ax_corr = myfig.add_subplot(1, 3, 2)\n",
    "plt.ylabel(\"Correlation coefficient\")\n",
    "plt.xlabel(\"Lambda values\")\n",
    "ax_corr.set_xscale('log')\n",
    "ax_corr.plot(todo_lambda, corr_overlam)\n",
    "\n",
    "ax_spar = myfig.add_subplot(1, 3, 3)\n",
    "plt.ylabel(\"Number of non-zero weights\")\n",
    "plt.xlabel(\"Lambda values\")\n",
    "ax_spar.set_xscale('log')\n",
    "ax_spar.plot(todo_lambda, spar_overlam)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "明らかなように、これほど単純なモデルと恣意的な学習アルゴリズムの設定では、まったく予測ができない。\n",
    "\n",
    "さまざまな要因はあるが、特に注視すべき改善点は下記の通りである。\n",
    "\n",
    "- 初期値`w_init`の決め方。\n",
    "\n",
    "- $\\lambda$の候補の範囲と密度。\n",
    "\n",
    "- 学習機の反復回数の制限`t_max`。\n",
    "\n",
    "- フィルタバンクを定めるあらゆるパラメータが大事だが、特に重要なのは`freqs`、`dir`、`sdev`であろう。\n",
    "\n",
    "- フィルタバンクの豊富さ。グリッドの解像度の高低、多様な空間周波数、向きなどをまんべんなく含むことが必須。\n",
    "\n",
    "これらの改善点を念頭において、次の「課題一覧」の練習問題に取り組んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tasks\"></a>\n",
    "## 課題一覧\n",
    "\n",
    "0. すべてのボクセル分の学習をする必要はなく、Nishimoto et al.のいう「early visual areas」に焦点を当てること。それは両半球においてV1、V2、V3、V3A、V3BというROIのことを指す。訓練データを使い、これらのすべてのクリーンなボクセルを対象に学習させ、検証データにおいて、訓練データでもっとも良かった$\\lambda$のときの学習結果の成績を算出すること。ROIごとに、これらの成績（相関係数等）のボクセルの上での平均と分散を求めること。\n",
    "\n",
    "0. 特徴量の配列を連結させることで、様々なフィルタからの特徴量を利用することができる。実際、良い説明能力を実現しようと思えば、おそらく不可欠な作業であろう。引用している論文でも、多数の空間周波数と向きの組み合わせが使われている。課題として、互いに異なる特徴量を捉えるフィルタバンクを２つ以上用意し、それぞれを学習用の使うこと。そのROIごとに結果を踏まえて、どの領域でどのような情報が必要かと思われるか。また、脳の部位の「選択性」について何がいえるか。\n",
    "\n",
    "0. 先の課題を、全被験者分のデータを使って行なうこと。被験者の間、同じモデルとアルゴリズムを使って、パフォーマンスに差異があるか。被験者の上でも平均的な成績を求めて、引用している論文の成績と勝負できるか。\n",
    "\n",
    "0. 手法の更なる強化を測るため、時間の遅延を利用して特徴量を作ることができる。つまり、前の時点の特徴量ベクトルを、現時点の特徴量ベクトルに連結させるという方法。現時点の特徴量を$x_{i}$とすると、$x_{i-1}$が前の時点に相当するので、連結した結果が$\\widetilde{x}_{i} = (x_{i},x_{i-1})$になって、この$\\widetilde{x}_{i}$を学習に使うことになる。遅延が$k$時点なら、$\\widetilde{x}_{i} = (x_{i},x_{i-1},\\ldots,x_{i-k})$となる。数ステップを試して、パフォーマンスがもっとも良い遅延はいくらか。\n",
    "\n",
    "0. 学習結果の良し悪しがROIによってどの程度変わるか。特段良い・悪い成績が見られたROIがあれば、それはどれか。\n",
    "\n",
    "0. モデルは大事だが、学習アルゴリズムもきわめて大きな役割を果たす。ベストな設定を探るべく、反復回数などの終了条件や$\\lambda$の候補範囲、その他のアルゴリズムの工夫を調べること。どのようなアルゴリズム設定がもっとも良い成績につながったか。独自の改造を行った場合、それも併せて説明すること。\n",
    "\n",
    "0. （おまけ）これまでは同一被験者を前提として、「汎化能力」を評価してきた。当然ながら、被験者間の汎化能力も注目に値する。つまり、2名の被験者のデータを足し合わせて、大きなデータセットにまとめ、訓練データとして使って学習してから、残り1名の被験者のデータを検証データとする。同じモデルとアルゴリズムでも十分なのか。それとも、新たな工夫が必要だったか。また、このときの汎化能力の解釈が、被験者ごとに学習する場合と比べて、どう違うか。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 参考文献：\n",
    "\n",
    " - Nishimoto, Shinji, et al. \"Reconstructing visual experiences from brain activity evoked by natural movies.\" Current Biology 21.19 (2011): 1641-1646.\n",
    " - Description of dataset vim-2 (visual imaging 2), at CRCNS - Collaborative Research in Computational Neuroscience. https://crcns.org/data-sets/vc/vim-2/about-vim-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
