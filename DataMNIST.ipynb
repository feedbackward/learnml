{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of raw (binary) data\n",
    "\n",
    "Raw data comes in many forms, often binary. Working with it is straightforward, but requires a certain degree of care to ensure that the data that we read in fact contains the information we expect.\n",
    "\n",
    "__Contents:__\n",
    "- <a href=\"#overview\">Overview of the data set</a>\n",
    "- <a href=\"#input_check\">Examining the input patterns</a>\n",
    "- <a href=\"#label_check\">Examining the instance labels</a>\n",
    "- <a href=\"#finalprep\">Final preparation of the data</a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Overview of the data set\n",
    "\n",
    "As a straightforward exercise in manipulating binary files using standard Python functions, here we shall make use of the the well-known database of handwritten digits, called MNIST, a __m__odified subset of a larger dataset from the __N__ational __I__nstitute of __S__tandards and __T__echnology.\n",
    "\n",
    "<img src=\"img/ex_MNIST.png\" alt=\"Stimuli Image\" />\n",
    "\n",
    "A typical source for this data set is the website of Y. LeCun (http://yann.lecun.com/exdb/mnist/). They provide the following description,\n",
    "\n",
    "> *\"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\"*\n",
    "\n",
    "and the following files containing the data of interest.\n",
    "\n",
    "> train-images-idx3-ubyte.gz:  training set images (9912422 bytes)\n",
    "\n",
    "> train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)\n",
    "\n",
    "> t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)\n",
    "\n",
    "> t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)\n",
    "\n",
    "The files are stored in a binary format called __IDX__, typically used for storing vector data. First we decompress the files via\n",
    "\n",
    "```\n",
    "$ cd data/MNIST\n",
    "$ gunzip train-images-idx3-ubyte.gz\n",
    "$ gunzip train-labels-idx1-ubyte.gz\n",
    "$ gunzip t10k-images-idx3-ubyte.gz\n",
    "$ gunzip t10k-labels-idx1-ubyte.gz\n",
    "```\n",
    "which leaves us with the desired binary files.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"input_check\"></a>\n",
    "## Examining the input patterns\n",
    "\n",
    "Let us begin by opening a file connection with the training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "toread = \"data/MNIST/train-images-idx3-ubyte\"\n",
    "\n",
    "f_bin = open(toread, mode=\"rb\")\n",
    "\n",
    "print(f_bin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to ensure that we are reading the data correctly, the only way to confirm this is by inspecting and checking with what the authors of the data file tell us *should* be there. From the page of LeCun et al. linked above, we have the following:\n",
    "\n",
    "```\n",
    "TRAINING SET IMAGE FILE (train-images-idx3-ubyte):\n",
    "[offset] [type]          [value]          [description]\n",
    "0000     32 bit integer  0x00000803(2051) magic number\n",
    "0004     32 bit integer  60000            number of images\n",
    "0008     32 bit integer  28               number of rows\n",
    "0012     32 bit integer  28               number of columns\n",
    "0016     unsigned byte   ??               pixel\n",
    "0017     unsigned byte   ??               pixel\n",
    "........\n",
    "xxxx     unsigned byte   ??               pixel\n",
    "```\n",
    "\n",
    "The \"offset\" here refers to the number of bytes read from the start of the file. An offset of zero refers to the first byte, and an offset of 0004 refers to the fifth byte, 0008 the ninth byte, and so forth. Let's check that we are able to successfully read what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"First four bytes:\") # should be magic number, 2051.\n",
    "b = f_bin.read(4)\n",
    "print(\"bytes: \", b)\n",
    "print(\" int: \", int.from_bytes(b, byteorder=\"big\", signed=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the byte data `b'\\x00\\x00\\x08\\x03'` shown here by Python is a hexadecimal representation of the first four bytes. This corresponds directly to the \"value\" in the first row of the table above, ``0x00000803``. The ``\\x`` breaks simply show where one byte starts and another ends, recalling that using two hexadecimal digits we can represent the integers from $0, 1, 2, \\ldots$ through to $(15 \\times 16^{1} + 15 \\times 16^{0}) = 255$, just as we can with 8 binary digits, or 8 *bits*. Anyways, converting this to decimal, $3 \\times 16^{0} + 8 \\times 16^{2} = 2051$, precisely what we expect.\n",
    "\n",
    "Using the *read* method, let us read four bits at a time to ensure the remaining data is read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Second four bytes:\") # should be number of imgs = 60000\n",
    "b = f_bin.read(4)\n",
    "print(\"bytes: \", b)\n",
    "print(\" int: \", int.from_bytes(b, byteorder=\"big\", signed=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Third four bytes:\") # should be number of rows = 28\n",
    "b = f_bin.read(4)\n",
    "print(\"bytes: \", b)\n",
    "print(\" int: \", int.from_bytes(b, byteorder=\"big\", signed=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Fourth four bytes:\") # should be number of cols = 28\n",
    "b = f_bin.read(4)\n",
    "print(\"bytes: \", b)\n",
    "print(\" int: \", int.from_bytes(b, byteorder=\"big\", signed=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things seem to be as they should be. We have been able to accurately extract all the information necessary to read out all the remaining data stored in this file. Since these happen to be images, the accuracy of our read-out can be easily assessed by looking at the image content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n = 60000 # (anticipated) number of images.\n",
    "d = 28*28 # number of entries (int values) per image.\n",
    "times_todo = 5 # number of images to view.\n",
    "bytes_left = d\n",
    "data_x = np.zeros((d,), dtype=np.uint8) # initialize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using the `uint8` (unsigned 1-byte int) data type, because we know that the values range between 0 and 255, based on the description by the authors of the data set, which says\n",
    "\n",
    "> \"Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).\"\n",
    "\n",
    "More concretely, we have that the remaining elements of the data set are of the form\n",
    "\n",
    "```\n",
    "0016     unsigned byte   ??               pixel\n",
    "0017     unsigned byte   ??               pixel\n",
    "........\n",
    "xxxx     unsigned byte   ??               pixel\n",
    "```\n",
    "and so to read out one pixel value at a time (values between 0 and 255), we must read one byte at a time (rather than four bytes as we had just been doing). There should be $28 \\times 28 = 784$ pixels per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for t in range(times_todo):\n",
    "\n",
    "    idx = 0\n",
    "    while idx < bytes_left:\n",
    "        # Iterate one byte at a time.\n",
    "        b = f_bin.read(1)\n",
    "        data_x[idx] = int.from_bytes(b, byteorder=\"big\", signed=False)\n",
    "        idx += 1\n",
    "\n",
    "    img_x = data_x.reshape( (28,28) ) # populate one row at a time.\n",
    "    \n",
    "    # binary colour map highlights foreground (black) against background(white)\n",
    "    plt.imshow(img_x, cmap=plt.cm.binary)\n",
    "    #plt.savefig((\"MNIST_train_\"+str(t)+\".png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "f_bin.close()\n",
    "if f_bin.closed:\n",
    "    print(\"Successfully closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises (A):\n",
    "\n",
    "0. When using the `from_bytes` method of `int`, change `signed` from `False` to `True`. Does the result of the binary to integer conversion change? If so, how? (provide examples) If possible, explain what causes this difference.\n",
    "\n",
    "0. Similarly, change `byteorder` from `\"big\"` to `\"little\"`, and investigate if and how things change. Check `help(int.from_bytes)` for more information.\n",
    "\n",
    "0. Note that there are countless colour maps (https://matplotlib.org/users/colormaps.html) available. Instead of `binary` as used, above try `gray`, `bone`, `pink`, and any others that catch your interest.\n",
    "\n",
    "0. Uncomment the `savefig` line above, and save the first 10 training images to file. Then do the exact same procedure for *test* images, changing the file names appropriately.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"label_check\"></a>\n",
    "## Examining the instance labels\n",
    "\n",
    "The images, or more generally, the *instances* to be used for the classification task appear as we expect. Let us now shift our focus over to the corresponding *labels* and confirm that the first `times_todo` instances indeed have the labels that we expect. These are stored in the `train-labels-idx1-ubyte` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toread = \"data/MNIST/train-labels-idx1-ubyte\"\n",
    "\n",
    "f_bin = open(toread, mode=\"rb\")\n",
    "\n",
    "print(f_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again from the page of LeCun et al. linked above, we have for labels that the contents should be as follows.\n",
    "\n",
    "```\n",
    "TRAINING SET LABEL FILE (train-labels-idx1-ubyte):\n",
    "[offset] [type]          [value]          [description]\n",
    "0000     32 bit integer  0x00000801(2049) magic number (MSB first)\n",
    "0004     32 bit integer  60000            number of items\n",
    "0008     unsigned byte   ??               label\n",
    "0009     unsigned byte   ??               label\n",
    "........\n",
    "xxxx     unsigned byte   ??               label\n",
    "```\n",
    "\n",
    "Let's inspect the first eight bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"First four bytes:\") # should be magic number, 2049.\n",
    "b = f_bin.read(4)\n",
    "print(\"bytes: \", b)\n",
    "print(\" int: \", int.from_bytes(b, byteorder=\"big\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Second four bytes:\") # should be number of observations, 60000.\n",
    "b = f_bin.read(4)\n",
    "print(\"bytes: \", b)\n",
    "print(\" int: \", int.from_bytes(b, byteorder=\"big\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here should be labels from 0 to 9. Let's confirm that the patterns above have correct labels corresponding to them in the place that we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for t in range(times_todo):\n",
    "\n",
    "    b = f_bin.read(1)\n",
    "    mylabel = int.from_bytes(b, byteorder=\"big\", signed=False)\n",
    "    \n",
    "    print(\"Label =\", mylabel)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises (B):\n",
    "\n",
    "0. Print out the label values for the first 10 images in both the training and testing data sets. Do these match the numbers that appear to be written in the images you saved to disk previously? (they should)\n",
    "\n",
    "0. (Bonus) Instead of using `read`, we can use `seek` to jump to an arbitrary byte offset. In the example of instance labels above, `f_bin.seek(0)` would take us to the start of the file, and `f_bin.seek(8)` takes us to the point where the first label value is saved (since the first 8 bytes are clerical information). With this in mind, write a function that uses `seek` to display the label of the $k$th image, given integer $k$ only.\n",
    "\n",
    "0. (Bonus) Similarly, write a function which uses `seek` to read and display the $k$th image itself, given just integer $k$.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"finalprep\"></a>\n",
    "## Final preparation of the data\n",
    "\n",
    "While our examination of the data thus far has not been completely rigorous (i.e., there are many image-label pairs we have not checked), the above analysis provides a good introduction to the kind of information we are dealing with in this (and similar) tasks, and the format in which that information is stored.\n",
    "\n",
    "Now, as an important practical concern, we will not want to have to open up the file and read pixel values byte-by-byte in the above fashion every time we want to train a classifier. A more reasonable approach is to read out each of the full data sets just once, and then re-write them to disk as a Python-format binary file, which is much faster to read on-the-fly, assuming the machine doing the *writing* is also the machine doing the *reading*.\n",
    "\n",
    "Let us start by reading the whole file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toread = \"data/MNIST/train-images-idx3-ubyte\"\n",
    "n = 60000\n",
    "d = 28*28\n",
    "bytes_left = n * d\n",
    "data_X = np.empty((n*d,), dtype=np.uint8)\n",
    "\n",
    "with open(toread, mode=\"rb\") as f_bin:\n",
    "\n",
    "    f_bin.seek(16) # go to start of images.\n",
    "    idx = 0\n",
    "    \n",
    "    print(\"Reading binary file...\", end=\" \")\n",
    "    while bytes_left > 0:\n",
    "        b = f_bin.read(1)\n",
    "        data_X[idx] = int.from_bytes(b, byteorder=\"big\", signed=False)\n",
    "        bytes_left -= 1\n",
    "        idx += 1\n",
    "    print(\"Done reading...\", end=\" \")\n",
    "print(\"OK, file closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the unsigned integer `unint8` data type, we have assembled all pixel values for all instances in one long vector. Let's examine basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Min:\", np.min(data_X))\n",
    "print(\"Mean:\", np.mean(data_X))\n",
    "print(\"Median:\", np.median(data_X))\n",
    "print(\"Max:\", np.max(data_X))\n",
    "print(\"StdDev:\", np.std(data_X))\n",
    "\n",
    "print(np.bincount(data_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain models run into numerical difficulties if the input features are too large. Here they range over $\\{0,1,\\ldots,255\\}$, which can lead to huge values when, for example, passed through exponential functions (e.g., logistic regression model).\n",
    "\n",
    "To get around this, it is useful to map the values to the unit interval $[0,1]$.<br>\n",
    "The basic formula is simple: (VALUE - MIN) / (MAX - MIN). It requires more memory to store floating-point numbers (in this case, four times more), but computation is often made considerably easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_X_float = np.empty(data_X.shape, dtype=np.float32)\n",
    "data_X_float = np.float32((data_X - np.min(data_X))/(np.max(data_X) - np.min(data_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Min:\", np.min(data_X_float))\n",
    "print(\"Mean:\", np.mean(data_X_float))\n",
    "print(\"Median:\", np.median(data_X_float))\n",
    "print(\"Max:\", np.max(data_X_float))\n",
    "print(\"StdDev:\", np.std(data_X_float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains now is to re-write to disk using the handy `tofile` function from `numpy`, as follows. The important point here is to ensure that we know the `dtype` used when writing, so that we can correctly read the data using the companion function `towrite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Writing binary file...\", end=\" \")\n",
    "towrite = \"data/MNIST/X_tr.dat\"\n",
    "with open(towrite, mode=\"bw\") as g_bin:\n",
    "    data_X_float.tofile(g_bin) # don't forget the dtype used.\n",
    "print(\"OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try reading this file, and compare with the original data. There should be no errors in reconstruction, and clearly reading from the Python-format binary file is *much* faster than reading from the IDX file one byte at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(towrite, mode=\"br\") as g_bin:\n",
    "    data_X_check = np.fromfile(g_bin, dtype=np.float32)\n",
    "print(\"OK.\")\n",
    "\n",
    "print(\"Shapes:\", data_X_check.shape, data_X_float.shape)\n",
    "print(\"Difference =\", np.linalg.norm(data_X_check-data_X_float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Let us do the same thing for label data. Instead of `X`, corresponding to input patterns, we call the labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toread = \"data/MNIST/train-labels-idx1-ubyte\"\n",
    "n = 60000\n",
    "bytes_left = n\n",
    "data_y = np.empty((n,), dtype=np.uint8)\n",
    "\n",
    "with open(toread, mode=\"rb\") as f_bin:\n",
    "\n",
    "    f_bin.seek(8) # go to start of the labels.\n",
    "    idx = 0\n",
    "    \n",
    "    print(\"Reading binary file...\", end=\" \")\n",
    "    while bytes_left > 0:\n",
    "        b = f_bin.read(1)\n",
    "        data_y[idx] = int.from_bytes(b, byteorder=\"big\", signed=False)\n",
    "        bytes_left -= 1\n",
    "        idx += 1\n",
    "    print(\"Done reading...\", end=\" \")\n",
    "print(\"OK, file closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we use the unsigned integer `unint8` data type, and assemble all the labels for the training data. Let's examine basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Min:\", np.min(data_y))\n",
    "print(\"Mean:\", np.mean(data_y))\n",
    "print(\"Median:\", np.median(data_y))\n",
    "print(\"Max:\", np.max(data_y))\n",
    "print(\"StdDev:\", np.std(data_y))\n",
    "\n",
    "print(\"Bin counts:\")\n",
    "print(np.bincount(data_y))\n",
    "\n",
    "plt.hist(np.hstack(data_y), bins='auto')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's repeat the process of writing and double-checking the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Writing binary file...\", end=\" \")\n",
    "towrite = \"data/MNIST/y_tr.dat\"\n",
    "with open(towrite, mode=\"bw\") as g_bin:\n",
    "    data_y.tofile(g_bin) # don't forget the dtype used.\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(towrite, mode=\"br\") as g_bin:\n",
    "    data_y_check = np.fromfile(g_bin, dtype=np.uint8)\n",
    "print(\"OK.\")\n",
    "\n",
    "print(\"Shapes:\", data_y_check.shape, data_y.shape)\n",
    "print(\"Difference =\", np.linalg.norm(data_y_check-data_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises (C):\n",
    "\n",
    "0. Repeat the above process for the testing data. We used `X_tr` and `y_tr` for naming the __tr__aining data; use `X_te` and `y_te` for naming the __te__sting data, saving each of their respective `.dat` files to the directory shown above.\n",
    "\n",
    "0. Save the histograms for both the training and test labels. Are the data sets \"balanced\" in terms of the labels present? Which digit is most common? Least common? Do these change between the training and test data sets?\n",
    "\n",
    "0. (Bonus) It is often convenient to \"centre\" and \"standardize\" data observations to have zero (empirical) mean and unit (empirical) variance. Use the `reshape` method to transform the long `data_X_float` vector into a matrix (a numpy array with two axes) of the form $n \\times d$, where $n$ is the number of samples, and $d$ is the number of pixels per image. Compute the per-column means and standard deviations, and subtract/divide as necessary.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of lesson: paste any routines to be re-used in the `scripts/MNIST.py` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
