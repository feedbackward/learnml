{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finishing the encoder\n",
    "\n",
    "__Contents:__\n",
    "\n",
    "- <a href=\"#raw2feat\">Create features from raw stimulus</a>\n",
    "- <a href=\"#algorun\">Initialize model and execute algorithm</a>\n",
    "- <a href=\"#eval\">Evaluate performance</a>\n",
    "- <a href=\"#tasks\">List of major tasks</a>\n",
    "\n",
    "___\n",
    "\n",
    "To the end of putting all the pieces together to get a fully-functional learning machine, let's load up the functions we prepared in other lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import scripts.FilterBank as fb\n",
    "import scripts.AlgoIntro as ai\n",
    "import scripts.AlgoSparseReg as asr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic workflow of the final procedure is as follows:\n",
    "\n",
    "    Load raw stimulus --> Generate and save visual features --> Fit sparse linear model.\n",
    "\n",
    "We shall take this one piece at a time. After going through a simple and fast prototypical implementation of this encoder, we will put forward a series of exercises that constitute the bulk of the work to be done here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"raw2feat\"></a>\n",
    "## Create features from raw stimulus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Import the vim-2 data from the Python binary format we saved earlier.\n",
    "# Assumptions: that the data is saved in the vim-2 directory,\n",
    "# already of 96x96 size, with dtype of np.float32.\n",
    "\n",
    "PIX_W = 96\n",
    "PIX_H = 96\n",
    "dtype=np.float32\n",
    "\n",
    "# Read the raw training data.\n",
    "shape=(PIX_W,PIX_H,3,108000) # (downsized px, downsized px, rgd channels, time steps)\n",
    "# Index for temporal down-sampling. Alternatives: do after the feature-building and aggregate.\n",
    "idx_ds = np.arange(15//2, 108000+1, 15) # need length 7200.\n",
    "fname = \"data/vim-2/X_tr.dat\"\n",
    "with open(fname, mode=\"br\") as fbin:\n",
    "    print(\"Reading...\", end=\" \")\n",
    "    raw_tr = np.fromfile(file=fbin,dtype=dtype).reshape(shape)[:,:,:,idx_ds] # temporally down-sampled.\n",
    "    print(\"OK.\")\n",
    "\n",
    "# Check a few frames.\n",
    "#num_frames = raw_tr.shape[3]\n",
    "#frames_to_play = 5\n",
    "#for t in range(frames_to_play):\n",
    "#    plt.imshow(raw_tr[:,:,:,t])\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "# Read the raw testing data.\n",
    "shape=(PIX_W,PIX_H,3,8100) # (downsized px, downsized px, rgd channels, time steps)\n",
    "# Index for temporal down-sampling. Alternatives: do after the feature-building and aggregate.\n",
    "idx_ds = np.arange(15//2, 8100+1, 15) # need length 540.\n",
    "fname = \"data/vim-2/X_te.dat\"\n",
    "with open(fname, mode=\"br\") as fbin:\n",
    "    print(\"Reading...\", end=\" \")\n",
    "    raw_te = np.fromfile(file=fbin, dtype=dtype).reshape(shape)[:,:,:,idx_ds] # temporally down-sampled.\n",
    "    print(\"OK.\")\n",
    "\n",
    "# Check a few frames.\n",
    "#num_frames = raw_tr.shape[3]\n",
    "#frames_to_play = 5\n",
    "#for t in range(frames_to_play):\n",
    "#    plt.imshow(raw_tr[:,:,:,t])\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up the parameters that specify the first filter bank.\n",
    "myparas = {\"freqs\": 32/max(PIX_W,PIX_H),\n",
    "           \"dir\": math.pi/2,\n",
    "           \"amp\": 0.1,\n",
    "           \"sdev\": max(PIX_W,PIX_H)/20,\n",
    "           \"phase\": 0}\n",
    "mygrid_h = 3\n",
    "mygrid_w = 3\n",
    "\n",
    "print(\"Getting features (tr)...\", end=\" \")\n",
    "X_tr = fb.G2_getfeatures(ims=raw_tr,\n",
    "                         fil_paras=myparas,\n",
    "                         gridshape=(mygrid_h,mygrid_w),\n",
    "                         mode=\"reflect\", cval=0)\n",
    "print(\"OK.\")\n",
    "\n",
    "print(\"Getting features (te)...\", end=\" \")\n",
    "X_te = fb.G2_getfeatures(ims=raw_te,\n",
    "                         fil_paras=myparas,\n",
    "                         gridshape=(mygrid_h,mygrid_w),\n",
    "                         mode=\"reflect\", cval=0)\n",
    "print(\"OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the produced features:\")\n",
    "print(\"tr:\", X_tr.shape)\n",
    "print(\"te:\", X_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above parameters only give us a very limited number of features. As a simple example, let's prepare another set of features determined by a qualitatively distinct collection of filters, and concatenate these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters that specify the second filter bank.\n",
    "myparas = {\"freqs\": 32/max(PIX_W,PIX_H),\n",
    "           \"dir\": 0,\n",
    "           \"amp\": 0.1,\n",
    "           \"sdev\": max(PIX_W,PIX_H)/20,\n",
    "           \"phase\": 0}\n",
    "mygrid_h = 9\n",
    "mygrid_w = 9\n",
    "\n",
    "print(\"Getting features (tr)...\", end=\" \")\n",
    "tmp_X = fb.G2_getfeatures(ims=raw_tr,\n",
    "                         fil_paras=myparas,\n",
    "                         gridshape=(mygrid_h,mygrid_w),\n",
    "                         mode=\"reflect\", cval=0)\n",
    "print(\"OK.\")\n",
    "X_tr = np.concatenate((X_tr, tmp_X), axis=1) # concatenate!\n",
    "\n",
    "print(\"Getting features (te)...\", end=\" \")\n",
    "tmp_X = fb.G2_getfeatures(ims=raw_te,\n",
    "                         fil_paras=myparas,\n",
    "                         gridshape=(mygrid_h,mygrid_w),\n",
    "                         mode=\"reflect\", cval=0)\n",
    "print(\"OK.\")\n",
    "X_te = np.concatenate((X_te, tmp_X), axis=1) # concatenate!\n",
    "\n",
    "print(\"Shape of the produced features:\")\n",
    "print(\"tr:\", X_tr.shape)\n",
    "print(\"te:\", X_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the concatenation just adds columns of feature vectors; the number of samples (and thus rows in the array) is constant.\n",
    "\n",
    "Again following Nishimoto et al. (2011), let us compute the so-called Z-score of the feature values, in order to center the mean and standardize the scale. In addition, a hard truncation of outliers is carried out. This is done separately for training and testing data, to ensure the learner does not gain unfair oracle information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_tr = X_tr - np.mean(X_tr, axis=0)\n",
    "X_tr = X_tr / np.std(X_tr, axis=0)\n",
    "print(\"Mean =\", np.mean(X_tr, axis=0), \"StdDev =\", np.std(X_tr, axis=0))\n",
    "\n",
    "X_te = X_te - np.mean(X_te, axis=0)\n",
    "X_te = X_te / np.std(X_te, axis=0)\n",
    "print(\"Mean =\", np.mean(X_te, axis=0), \"StdDev =\", np.std(X_te, axis=0))\n",
    "\n",
    "for j in range(X_tr.shape[1]):\n",
    "    stdval = np.std(X_tr[:,j])\n",
    "    X_tr[:,j] = np.clip(X_tr[:,j], a_min=(-stdval), a_max=stdval)\n",
    "    stdval = np.std(X_te[:,j])\n",
    "    X_te[:,j] = np.clip(X_te[:,j], a_min=(-stdval), a_max=stdval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming everything worked up to this point, write the features to disk, and at the same time create a corresponding \"data info\" object, also written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import support.classes as classes\n",
    "\n",
    "# Make a data_info file for this data, and save as data/encoder/info.dat.\n",
    "dinfo = classes.DataInfo()\n",
    "dinfo.mname = \"Encoder\"\n",
    "\n",
    "fname = \"data/encoder/X_tr.dat\"\n",
    "dtype = raw_tr.dtype\n",
    "shape = raw_tr.shape\n",
    "with open(fname, mode=\"bw\") as fbin:\n",
    "    X_tr.tofile(fbin)\n",
    "    print(\"Saved to file.\")\n",
    "dinfo.X_tr[\"shape\"] = X_tr.shape\n",
    "dinfo.X_tr[\"path\"] = \"data/encoder/X_tr.dat\"\n",
    "dinfo.X_tr[\"dtype\"] = X_tr.dtype\n",
    "    \n",
    "fname = \"data/encoder/X_te.dat\"\n",
    "dtype = raw_te.dtype\n",
    "shape = raw_te.shape\n",
    "with open(fname, mode=\"bw\") as fbin:\n",
    "    X_te.tofile(fbin)\n",
    "    print(\"Saved to file.\")\n",
    "dinfo.X_te[\"shape\"] = X_te.shape\n",
    "dinfo.X_te[\"path\"] = \"data/encoder/X_te.dat\"\n",
    "dinfo.X_te[\"dtype\"] = X_te.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import support.classes as classes\n",
    "\n",
    "# Make a data_info file for this data, and save as data/encoder/info.dat.\n",
    "dinfo = classes.DataInfo()\n",
    "dinfo.mname = \"Encoder\"\n",
    "\n",
    "fname = \"data/encoder/X_tr.dat\"\n",
    "dtype = raw_tr.dtype\n",
    "shape = raw_tr.shape\n",
    "with open(fname, mode=\"bw\") as fbin:\n",
    "    X_tr.tofile(fbin)\n",
    "    print(\"Saved to file.\")\n",
    "dinfo.X_tr[\"shape\"] = X_tr.shape\n",
    "dinfo.X_tr[\"path\"] = \"data/encoder/X_tr.dat\"\n",
    "dinfo.X_tr[\"dtype\"] = X_tr.dtype\n",
    "\n",
    "fname = \"data/encoder/X_te.dat\"\n",
    "dtype = raw_te.dtype\n",
    "shape = raw_te.shape\n",
    "with open(fname, mode=\"bw\") as fbin:\n",
    "    X_te.tofile(fbin)\n",
    "    print(\"Saved to file.\")\n",
    "dinfo.X_te[\"shape\"] = X_te.shape\n",
    "dinfo.X_te[\"path\"] = \"data/encoder/X_te.dat\"\n",
    "dinfo.X_te[\"dtype\"] = X_te.dtype\n",
    "\n",
    "# Clear the raw data from memory.\n",
    "del [raw_te, raw_tr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the responses (and the index of voxels free of missing values), just move or copy them over to our `encoder` directory,\n",
    "\n",
    "```\n",
    "$ mv data/vim-2/y_tr.dat data/vim-2/y_te.dat ./data/encoder/\n",
    "$ mv data/vim-2/cleanidx_tr.dat data/vim-2/cleanidx_te.dat ./data/encoder/\n",
    "\n",
    "```\n",
    "\n",
    "and then similarly populate the remaining attributes of `dinfo` and write to disk, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "fname = \"data/encoder/y_tr.dat\"\n",
    "dinfo.y_tr[\"shape\"] = (73728, 7200)\n",
    "dinfo.y_tr[\"path\"] = \"data/encoder/y_tr.dat\"\n",
    "dinfo.y_tr[\"dtype\"] = np.float32\n",
    "\n",
    "fname = \"data/encoder/y_te.dat\"\n",
    "dinfo.y_te[\"shape\"] = (73728, 540)\n",
    "dinfo.y_te[\"path\"] = \"data/encoder/y_te.dat\"\n",
    "dinfo.y_te[\"dtype\"] = np.float32\n",
    "\n",
    "dinfo.misc = {\"voxidx\": None} # to be filled in later.\n",
    "\n",
    "with open(\"data/encoder/info.dat\", mode=\"bw\") as fbin:\n",
    "    pickle.dump(dinfo, fbin)\n",
    "    print(\"Saved to file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"algorun\"></a>\n",
    "## Initialize model and execute algorithm\n",
    "\n",
    "Assuming the above has all been run once, then we can feel free to restart the kernel and run from here.\n",
    "\n",
    "We shall run the `Algo_LASSO_CD` routine implemented in a previous lesson here, over a grid of $\\lambda$ parameters controlling the impact of the $\\ell_{1}$ norm constraint. As for performance metrics, citing the Nishimoto et al. (2011) work, from which this data set was born:\n",
    "\n",
    "> *\"Prediction accuracy was defined as the correlation between predicted and observed BOLD signals. The averaged accuracy across subjects and voxels in early visual areas (V1, V2, V3, V3A, and V3B) was 0.24, 0.39, and 0.40 for the static, nondirectional, and directional encoding models, respectively.\"*\n",
    "\n",
    "Every element of our implementation is less sophisticated than theirs, from the filter bank used to create inputs, to the learning procedure used to set parameter values, and thus this performance should be considered an upper bound for the performance we can achieve in our pedagogical exercise here. In particular, since we are only using two-dimensional Gabor filters, our encoding model corresponds to a simplified version of their \"static\" encoding model (which achieved average accuracy of 0.24).\n",
    "\n",
    "After running our algorithm, as an output we get an estimate $\\widehat{w}$ called `w_est`. Given a new collection of features $X$ (this will be `X_te`) and a response $y$ (this will be `y_te`), the goal is to estimate as $\\widehat{y} \\approx y$ with $\\widehat{y} = X\\widehat{w}$. Evaluation of performance then can be done with the correlation\n",
    "\n",
    "\\begin{align}\n",
    "\\text{corr}\\,(\\widehat{y},y) = \\frac{\\text{cov}\\,(\\widehat{y},y)}{\\sqrt{\\text{var}\\,(\\widehat{y})\\text{var}\\,(y)}},\n",
    "\\end{align}\n",
    "\n",
    "implemented in `scipy.stats.pearsonr`, and the *root mean squared error* (RMSE), defined\n",
    "\n",
    "\\begin{align}\n",
    "\\text{RMSE}\\,(\\widehat{y},y) = \\left( \\frac{1}{m} \\sum_{i=1}^{m} (\\widehat{y}_{i}-y_{i})^2 \\right)^{1/2},\n",
    "\\end{align}\n",
    "\n",
    "implemented in `mod.eval` as a method of the model object, where $m$ represents the number of samples in the test data (here $m=540$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import support.parse_model as mp\n",
    "import scripts.AlgoSparseReg as asr\n",
    "\n",
    "# Load the general data info object.\n",
    "with open(\"data/encoder/info.dat\", mode=\"br\") as fbin:\n",
    "    dinfo = pickle.load(fbin)\n",
    "\n",
    "# Load the clean index, extracting indices from binary indicators.\n",
    "with open(\"data/encoder/cleanidx_tr.dat\", mode=\"br\") as fbin:\n",
    "    #cleanidx_tr_RAW = np.fromfile(file=fbin, dtype=np.uint32)\n",
    "    cleanidx_tr = np.flatnonzero(np.fromfile(file=fbin, dtype=np.uint32))\n",
    "    print(\"length:\", cleanidx_tr.size)\n",
    "with open(\"data/encoder/cleanidx_te.dat\", mode=\"br\") as fbin:\n",
    "    #cleanidx_te_RAW = np.fromfile(file=fbin, dtype=np.uint32)\n",
    "    cleanidx_te = np.flatnonzero(np.fromfile(file=fbin, dtype=np.uint32))\n",
    "    print(\"length:\", cleanidx_te.size)\n",
    "    \n",
    "# Take the intersection of the clean voxel indices and sort.\n",
    "cleanidx = np.intersect1d(cleanidx_tr,cleanidx_te)\n",
    "print(cleanidx_tr)\n",
    "print(cleanidx_te)\n",
    "print(\"cleanidx length:\", cleanidx.size)\n",
    "\n",
    "# Load the data info object. We shall modify its voxel index on the fly.\n",
    "with open(\"data/encoder/info.dat\", mode=\"br\") as fbin:\n",
    "    dinfo = pickle.load(fbin)\n",
    "    print(dinfo)\n",
    "\n",
    "# Initialize model and weights for an individual voxel.\n",
    "dinfo.misc[\"voxidx\"] = cleanidx[0]\n",
    "mod = mp.model(dinfo)\n",
    "w_init = mod.w_initialize()\n",
    "\n",
    "\n",
    "lam_min = 1 / mod.n\n",
    "\n",
    "# TODO: set the START and STOP guys with proper computations using mod.X_tr and mod.y_tr.\n",
    "todo_lambda = np.flipud(np.logspace(start=math.log10(lam_min),\n",
    "                                    stop=math.log10(1),\n",
    "                                    num=150))\n",
    "\n",
    "# Store performance metric statistics for each lambda setting.\n",
    "err_overlam = np.zeros(todo_lambda.size, dtype=np.float32)\n",
    "corr_overlam = np.zeros(todo_lambda.size, dtype=np.float32)\n",
    "spar_overlam = np.zeros(todo_lambda.size, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over the lambda values once, for a single candidate.\n",
    "print(\"Working...\")\n",
    "for i in range(todo_lambda.size):\n",
    "    \n",
    "    # Initialize and execute the algorithm.\n",
    "    al = asr.Algo_LASSO_CD(w_init=w_init,\\\n",
    "                           t_max=20*w_init.size,\\\n",
    "                           lam_l1=todo_lambda[i],\\\n",
    "                           verbose=False)\n",
    "    \n",
    "    for mystep in al:\n",
    "        al.update(model=mod)\n",
    "    \n",
    "    # Record performance.\n",
    "    w_est = al.w\n",
    "    err_overlam[i] = mod.eval(w_est)[0]\n",
    "    if np.std(np.dot(mod.X_te, w_est)) > 0:\n",
    "        corrval = mod.corr_te(w_est)\n",
    "    else:\n",
    "        corrval = 0  # watch out for zero-variance case.\n",
    "    corr_overlam[i] = corrval\n",
    "    spar_overlam[i] = np.count_nonzero(w_est)\n",
    "    \n",
    "    # Update the initializer to the most current observation.\n",
    "    w_init = w_est\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having run the routines, it is typically best to store the raw results for later processing and use. This is done in the `raw/encoder` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw/encoder/lasso01_lam.raw\", mode=\"bw\") as fbin:\n",
    "    pickle.dump(todo_lambda, fbin)\n",
    "    print(\"Saved to file.\")\n",
    "\n",
    "with open(\"raw/encoder/lasso01_err.raw\", mode=\"bw\") as fbin:\n",
    "    pickle.dump(err_overlam, fbin)\n",
    "    print(\"Saved to file.\")\n",
    "    \n",
    "with open(\"raw/encoder/lasso01_corr.raw\", mode=\"bw\") as fbin:\n",
    "    pickle.dump(corr_overlam, fbin)\n",
    "    print(\"Saved to file.\")\n",
    "    \n",
    "with open(\"raw/encoder/lasso01_spar.raw\", mode=\"bw\") as fbin:\n",
    "    pickle.dump(spar_overlam, fbin)\n",
    "    print(\"Saved to file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"eval\"></a>\n",
    "## Evaluate performance\n",
    "\n",
    "Feel free to restart the kernel here for evaluation, since the core results should all be saved to disk at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"raw/encoder/lasso01_lam.raw\", mode=\"br\") as fbin:\n",
    "    todo_lambda = pickle.load(fbin)\n",
    "\n",
    "with open(\"raw/encoder/lasso01_err.raw\", mode=\"br\") as fbin:\n",
    "    err_overlam = pickle.load(fbin)\n",
    "    \n",
    "with open(\"raw/encoder/lasso01_corr.raw\", mode=\"br\") as fbin:\n",
    "    corr_overlam = pickle.load(fbin)\n",
    "    \n",
    "with open(\"raw/encoder/lasso01_spar.raw\", mode=\"br\") as fbin:\n",
    "    spar_overlam = pickle.load(fbin)\n",
    "\n",
    "myfig = plt.figure(figsize=(18,4))\n",
    "ax_err = myfig.add_subplot(1, 3, 1)\n",
    "plt.ylabel(\"Root mean squared error (RMSE)\")\n",
    "plt.xlabel(\"Lambda values\")\n",
    "ax_err.set_yscale('log')\n",
    "ax_err.set_xscale('log')\n",
    "ax_err.plot(todo_lambda, err_overlam)\n",
    "\n",
    "ax_corr = myfig.add_subplot(1, 3, 2)\n",
    "plt.ylabel(\"Correlation coefficient\")\n",
    "plt.xlabel(\"Lambda values\")\n",
    "ax_corr.set_xscale('log')\n",
    "ax_corr.plot(todo_lambda, corr_overlam)\n",
    "\n",
    "ax_spar = myfig.add_subplot(1, 3, 3)\n",
    "plt.ylabel(\"Number of non-zero weights\")\n",
    "plt.xlabel(\"Lambda values\")\n",
    "ax_spar.set_xscale('log')\n",
    "ax_spar.plot(todo_lambda, spar_overlam)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that such a simple toy example is wildly inefficient---the output is basically junk. Some serious modifications to the following factors will be required. Keep them in mind when reading the major tasks below.\n",
    "\n",
    "- Setting of `w_init`.\n",
    "- The size and range of the $\\lambda$ grid.\n",
    "- Number of iterations `t_max`.\n",
    "- All of the filter bank parameters (in `myparas`), especially `freqs`, `dir`, `sdev`.\n",
    "- The quantity and variety of filters used to generate features; both fine and coarse grids, and a wide variety of frequencies and orientations is likely necessary (see the \"major tasks\" below for more ideas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tasks\"></a>\n",
    "## List of major tasks\n",
    "\n",
    "0. Focus on \"the early visual areas\" looked at by Nishimoto et al. (V1, V2, V3, V3A, and V3B) in both hemispheres. Train a model for each voxel in these regions, and compute the performance (on test data) for the best lambda value (determined on the *training* set). Average the error/correlation over all the voxels in these regions.\n",
    "\n",
    "0. By concatenating feature arrays, we can assemble features from a wide variety of filters. Indeed, this is almost certainly necessary for reasonable prediction quality. A wide variety of orientations and spatial frequencies was used in the cited neuroscience paper. Train several models (working as a team, perhaps) that use qualitatively *distinct* filter banks. Compare their respective performance, averaged across each of the early visual ROIs. Do you see any interesting trends? If so, what might these suggest about the \"selectivity\" of these regions of the brain?\n",
    "\n",
    "0. Complete the above exercise for each subject. Is there much difference in performance between subjects? How does your best model perform against the cited work?\n",
    "\n",
    "0. Another approach is to capture temporal delays in the feature vectors. For example, if our original feature vectors are $x_{i}$ for $i=1,\\ldots,n$, then re-christen the feature vectors as $\\widetilde{x}_{i} = (x_{i},x_{i-1})$ for a delay of *one* step (here, one second) for all $i>1$ (we lose one data point, now $n-1$ total). The dimension grows from $d$ to $2d$. Analogously, for a delay of $k$ steps, this would be $\\widetilde{x}_{i} = (x_{i},x_{i-1},\\ldots,x_{i-k})$ for all $i>k$, and lose $k$ data points for a total of $n-k$ now. Try several temporal delays; which seem to work best?\n",
    "\n",
    "0. How does performance depend on ROI looked at? Which ROI saw comparatively good/bad performance? Provide visuals to highlight performance in each region.\n",
    "\n",
    "0. Be sure to experiment with the learning algorithm parameters (number of iterations, size and range of $\\lambda$ grid, etc.). What strategies did you find particularly effective? If you made any modifications to the algorithm, describe them.\n",
    "\n",
    "0. (Bonus) Make a large data set of stimulus from any two subjects, and use the third subject's data as a evaluation of *inter-subject generalization* ability. How does performance compare with the more standard by-subject approach? Are there subjects that are particularly difficult to predict for? In contrast to this, considering the by-subject training approach we have considered thus far, how does our interpretation of *generalization* change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References:\n",
    "\n",
    " - Nishimoto, Shinji, et al. \"Reconstructing visual experiences from brain activity evoked by natural movies.\" Current Biology 21.19 (2011): 1641-1646.\n",
    " - Description of dataset vim-2 (visual imaging 2), at CRCNS - Collaborative Research in Computational Neuroscience. https://crcns.org/data-sets/vc/vim-2/about-vim-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
