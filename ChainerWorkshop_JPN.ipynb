{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習APIで学習機の実装を加速させる\n",
    "\n",
    "__目次__\n",
    "\n",
    "- <a href=\"#AD_bg\">自動微分について</a>\n",
    "  - <a href=\"#AD_bg_forwardone\">フォワードモード（1変数の例）</a>\n",
    "  - <a href=\"#AD_bg_forwardmulti\">フォワードモード（多変数の例）</a>\n",
    "  - <a href=\"#AD_bg_reverse\">リバースモード</a>\n",
    "- <a href=\"#NN_bg\">ニューラルネットワークと逆伝播法</a>\n",
    "- <a href=\"#chainer_bg\">Chainerを使ってみよう</a>\n",
    "  - <a href=\"#chainer_exSimple\">簡単な計算</a>\n",
    "  - <a href=\"#chainer_linreg\">線形回帰での最小二乗法を再現</a>\n",
    "  - <a href=\"#chainer_exNonLin\">任意の活性化関数を用いた非線形モデル</a>\n",
    "  - <a href=\"#chainer_lgstreg\">多クラスのロジスティック回帰の自作と比較</a>\n",
    "  - <a href=\"#chainer_newopt\">最適化と自分の手で</a>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"AD_bg\"></a>\n",
    "## 自動微分について\n",
    "\n",
    "一言でいうと、自動微分（automatic differentiation, AD）とは、連鎖律を巧妙に繰り返して使うことで、合成関数の微分計算を実装する多種の手法のことである。まず重要なことを述べておくと、\n",
    "\n",
    "- 数式処理も近似もせず、合成関数のパーツから解析的に求める。\n",
    "\n",
    "- 代表的なものとしてボトムアップ型もトップダウン型もあって、扱っている関数の入力と出力それぞれの数によって、どれが効率的かが違ってくる。\n",
    "\n",
    "さて、簡単な事例から見ていこう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"AD_bg_forwardone\"></a>\n",
    "### フォワードモード（1変数の例）\n",
    "\n",
    "まずは一変数の関数からなる単純な合成関数を例として取り上げて、ボトムアップ型の方法を見ていくことにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数$f(g(h(x)))$を考える。\n",
    "\n",
    "これらの$f,g,h$が$\\mathbb{R}$の上で微分可能で、実数値を返す関数とする。\n",
    "\n",
    "やりたいこと：$x$について、$f(g(h(x)))$の微分を求めたい（導関数の取る値を計算したい）。\n",
    "\n",
    "次の表現を使うと便利である。\n",
    "\n",
    "\\begin{align*}\n",
    "u_{0} & = x\\\\\n",
    "u_{1} & = h(u_{0})\\\\\n",
    "u_{2} & = g(u_{1})\\\\\n",
    "u_{3} & = f(u_{2}).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "連鎖律を何度か使うと、以下のように展開できる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d u_{3}}{d x} & = \\left(\\frac{d u_{3}}{d u_{2}}\\frac{d u_{2}}{d x}\\right)\\\\\n",
    "& = \\frac{d u_{3}}{d u_{2}}\\left(\\frac{d u_{2}}{d u_{1}}\\frac{d u_{1}}{d x}\\right)\\\\\n",
    "& = \\frac{d u_{3}}{d u_{2}}\\frac{d u_{2}}{d u_{1}} \\left(\\frac{d u_{1}}{d u_{0}}\\frac{d u_{0}}{d x}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "括弧の中にあるところに注視しながら、「層」ごとに入力$x$についての微分を次のように定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\dot{u}_{i} = \\frac{d u_{i}}{d x}, \\quad i = 0,1,\\ldots,3\n",
    "\\end{align*}\n",
    "\n",
    "明らかな通り、「下層」から始めると、次のように全層分を再帰的に計算できる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\dot{u}_{i} = \\frac{d u_{i}}{d u_{i-1}} \\dot{u}_{i-1}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードモードの自動微分のアルゴリズムをざっくりまとめると次の通りである。\n",
    "\n",
    "0. シードたる$\\dot{u}_{0}$を計算しておく（普通は$\\dot{u}_{0}=1$）\n",
    "\n",
    "0. 以降、$i=1,2,\\ldots$に対して：\n",
    "   0. $d u_{i} / d u_{i-1}$を計算\n",
    "   0. $\\dot{u}_{i-1}$を用いて、$\\dot{u}_{i}$を計算\n",
    "\n",
    "上記の通りに、有限個の計算を経て、$d f(g(h(x))) / dx$を正しく計算することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"AD_bg_forwardmulti\"></a>\n",
    "### フォワードモード（多変数の例）\n",
    "\n",
    "次は多変数への拡張を考える。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x} \\in \\mathbb{R}^{d_{0}}, \\enspace h:\\mathbb{R}^{d_{0}} \\to \\mathbb{R}^{d_{1}}, \\enspace g:\\mathbb{R}^{d_{1}} \\to \\mathbb{R}^{d_{2}}, \\enspace f:\\mathbb{R}^{d_{2}} \\to \\mathbb{R}^{d_{3}}\\end{align*}\n",
    "\n",
    "先ほどと同様に、いくつかの層に分けて、その入出力をベクトル表記すると以下のようになる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{u}_{0} & = \\mathbf{x}\\\\\n",
    "\\mathbf{u}_{1} & = h(\\mathbf{u}_{0})\\\\\n",
    "\\mathbf{u}_{2} & = g(\\mathbf{u}_{1})\\\\\n",
    "\\mathbf{u}_{3} & = f(\\mathbf{u}_{2}).\n",
    "\\end{align*}\n",
    "\n",
    "流れは特に変わらないが、ここでは通常の微分ではなく、__偏微分__である。偏微分をたくさん整列させたヤコビ行列が基本となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力を$\\mathbf{x} = (x_{1},\\ldots,x_{d_{0}})$、各層の出力を$\\mathbf{u}_{i}=(u_{i,1},\\ldots,u_{i,d_{i}})$と書く。次のように各層の出力を下層の入力について微分したヤコビ行列を表わす。\n",
    "\n",
    "\\begin{align*}\n",
    "\\dot{U}_{i} = \\left[ \\frac{\\partial u_{i,j}}{\\partial x_{k}} \\right]_{j,k},\n",
    "\\end{align*}\n",
    "\n",
    "この行列の形は$(d_{i} \\times d_{0})$である。\n",
    "\n",
    "前節の肝ともいえる再帰的な演算をここで多次元へと拡張する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial u_{i,j}}{\\partial x_{k}} = \\sum_{l=1}^{d_{i-1}} \\frac{\\partial u_{i,j}}{\\partial u_{i-1,l}} \\frac{\\partial u_{i-1,l}}{\\partial x_{k}}\n",
    "\\end{align*}\n",
    "\n",
    "これは各$i=1,2\\ldots$と$j \\in [d_{i}]$に対して定義されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各層の入出力だけで出来上がる$(d_{i} \\times d_{i-1})$ヤコビ行列もある。\n",
    "\n",
    "\\begin{align*}\n",
    "J_{i} = \\left[\\frac{\\partial u_{i,j}}{\\partial u_{i-1,k}}\\right]_{j,k},\n",
    "\\end{align*}\n",
    "\n",
    "この2つをかけておくと、行列の掛け算による再帰的計算式が得られる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\dot{U}_{i} = J_{i}\\dot{U}_{i-1}.\n",
    "\\end{align*}\n",
    "\n",
    "今度はシードたるものが$\\dot{U}_{0}$で、普通は単位行列となるが、あとは先述の一変数のときとまったく同様に進む。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__練習問題__\n",
    "\n",
    "0. 以下の関数について「層」に分けて、計算の順序を明記すること。\n",
    "\n",
    "\\begin{align*}\n",
    "f(x_{1},x_{2}) & = x_{1}x_{2} + \\sin(x_{1})\\\\\n",
    "f(x_{1},x_{2}) & = x_{1}+x_{2} + 2\\exp(x_{1}x_{2})\\\\\n",
    "f(x_{1},x_{2}) & = \\frac{x_{1}^{3}}{x_{2}} + \\exp(\\sin(x_{1}))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"AD_bg_reverse\"></a>\n",
    "### リバースモード\n",
    "\n",
    "正方向のほうは入力から計算していくため自然でわかりやすいが、入力変数が多い場合は計算が大変である。一方のリバースモードは少し変わった計算をするが、フォワードモードが遅いときには活躍することが多い。\n",
    "\n",
    "前の例をここでも使うことにする。$y = f(g(h(x)))$という合成関数であったことを思い出して、連鎖律を駆使すると以下のように展開できる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d y}{d x} & = \\left(\\frac{d y}{d u_{0}} \\frac{d u_{0}}{d x}\\right)\\\\\n",
    "& = \\left(\\frac{d y}{d u_{1}} \\frac{d u_{1}}{d u_{0}}\\right) \\frac{d u_{0}}{d x}\\\\\n",
    "& = \\left(\\frac{d y}{d u_{2}} \\frac{d u_{2}}{d u_{1}}\\right) \\frac{d u_{1}}{d u_{0}} \\frac{d u_{0}}{d x}\\\\\n",
    "& = \\left(\\frac{d y}{d u_{3}} \\frac{d u_{3}}{d u_{2}}\\right) \\frac{d u_{2}}{d u_{1}} \\frac{d u_{1}}{d u_{0}} \\frac{d u_{0}}{d x}.\n",
    "\\end{align*}\n",
    "\n",
    "ここでもやはり括弧の中に注視しながら、今度は上層の出力を、任意の中間層の出力について微分を取ることが重要になる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{u}_{i} = \\frac{d y}{d u_{i}}, \\quad i=0,1,2,3.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の通り、この新しい微分を再帰的に求めることができる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{u}_{i} = \\bar{u}_{i+1} \\frac{d u_{i+1}}{d u_{i}}.\n",
    "\\end{align*}\n",
    "\n",
    "最上層のインデックスが$K$であるとすれば、リバースモードの自動微分は概ね次のように行われる。\n",
    "\n",
    "0. シードたる$\\bar{u}_{K}$を計算しておく（普通は$\\bar{u}_{K}=1$）\n",
    "0. 各$i=0,1,\\ldots,K-1$に対して：\n",
    "   0. $d u_{K-i} / d u_{K-i-1}$を計算\n",
    "   0. $\\bar{u}_{K-i}$を使って、$\\bar{u}_{K-i-1}$を計算\n",
    "\n",
    "正方向とはまさに逆で、上層から下りていくという流れである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多変数への拡張も想像できるように行なう。$\\mathbf{y} = \\mathbf{u}_{K}$とおくと、各層の出力で微分を求めると以下のようになる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial y_{j}}{\\partial u_{i,k}} = \\sum_{l=1}^{d_{i+1}} \\frac{\\partial y_{j}}{\\partial u_{i+1,l}} \\frac{\\partial u_{i+1,l}}{\\partial u_{i,k}}\n",
    "\\end{align*}\n",
    "\n",
    "これを各$i=0,1,\\ldots,K-1$と$k \\in [d_{i}]$に対して定義される。\n",
    "\n",
    "2種のヤコビ行列を以下のように表わす。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{U}_{i} & = \\left[ \\frac{\\partial y_{j}}{\\partial u_{i,k}} \\right]_{j,k} \\quad (d_{K} \\times d_{i}), \\quad i = 0,1,\\ldots,K \\\\\n",
    "\\widetilde{J}_{i} & = \\left[ \\frac{\\partial u_{i+1,j}}{\\partial u_{i,k}} \\right]_{j,k} \\quad (d_{i+1} \\times d_{i}), \\quad i=0,1,\\ldots,K-1.\n",
    "\\end{align*}\n",
    "\n",
    "シードが$\\bar{U}_{K}$で、普通は単位行列となるのだが、これさえあればあとは再帰的に「下りていく」だけである。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{U}_{i} = \\bar{U}_{i+1} \\widetilde{J}_{i}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習では、実数値を返すロス関数の勾配を求めること、つまり$d_{K}=1$となることが多い。\n",
    "\n",
    "<img src=\"img/mtx_mult_fromleft.png\"  alt=\"Matrix multiplication from left\" width=\"243\"  height=\"250\" />\n",
    "\n",
    "うまく実装すると、$d_{K} \\ll d_{0}$という状況下では、フォワードよりも、リバースのほうが格段に速い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"NN_bg\"></a>\n",
    "## ニューラルネットワークと逆伝播法\n",
    "\n",
    "ニューラルネットワークを定式化するにあたって、2つの関係式を用いることでほぼ何でも表せる。\n",
    "\n",
    "\\begin{align*}\n",
    "x_{j} & = \\sum_{i \\to j} w_{ij}y_{i}\\\\\n",
    "y_{j} & = f_{j}(x_{j})\n",
    "\\end{align*}\n",
    "\n",
    "この$j \\in \\mathcal{V}$がユニット（ノードなどとも）のインデックスである。\n",
    "\n",
    "- $x_{j}$は$j$番目のユニットへの入力。\n",
    "- $y_{j}$は$j$番目のユニットからの出力。\n",
    "- $f_{j}$は$j$番目のユニットの活性化関数。\n",
    "- \"$i \\to j$\"とは、$j$に接続する（直接$j$の入力に寄与する）すべてのユニット。\n",
    "\n",
    "「ユニット$i$がユニット$j$に接続する」ことを$e_{i,j}=1$で表し、そうでない場合は$e_{i,j}=0$とする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの「層」はユニットの部分集合である。\n",
    "\n",
    "\\begin{align*}\n",
    "V_{1}, V_{2}, \\ldots, V_{L} \\subseteq \\mathcal{V}.\n",
    "\\end{align*}\n",
    "\n",
    "簡明でよく使われる構造としては、フィードフォワードが代表的である。我々の記号を使うと、任意の$i \\in V_{l}$, $j \\in V_{m}$について、\n",
    "\n",
    "\\begin{align*}\n",
    "m \\geq l \\implies e_{j,i} = 0\n",
    "\\end{align*}\n",
    "\n",
    "と下層から上層へと計算をきれいに分離できるモデルである。\n",
    "\n",
    "- 一般には何でも良いが、よくある「fully connected」層とは、各$i \\in V_{l}$と$j \\in V_{l+1}$に対して、$e_{i,j}=1$が成り立つということである。\n",
    "\n",
    "- $m > l+1$でも$e_{i,j}=1$となることもある（\"skip-layer\"接続）。\n",
    "\n",
    "- 任意の$i$に対して$e_{i,j}=0$であれば、$j$を__入力ユニット__と呼ぶ。\n",
    "\n",
    "- 任意の$i$に対して$e_{j,i}=0$であれば、$j$を__出力ユニット__と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有名な活性化関数をいくつかピックアップ：\n",
    "\n",
    "- 線形：$f(x) = x$\n",
    "- 閾値：$f(x) = I\\{x > 0\\}$\n",
    "- ロジスティック：$f(x) = e^{x}/(1+e^{x})$\n",
    "- Rectified linear unit: $f(x) = \\max\\{0,x\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワークの重みが最適化の対象となることが多いが、そのためには何らかの目的関数が必要である。仮にそれを$L$と書く。出力層$O \\subset \\mathcal{V}$に直接的に依存することが多い。$\\ell_{2}$誤差（つまり$\\ell_{2}$ノルムの二乗をとったもの）が回帰問題では定番の一つである。\n",
    "\n",
    "\\begin{align*}\n",
    "L = \\sum_{l \\in O} (t_{l} - y_{l})^{2},\n",
    "\\end{align*}\n",
    "\n",
    "これらの$t_{l}$が出力層の各ユニットの値である。2018年現在、最適化法として、偏微分を求めて勾配ベクトルだけを使った反復的な更新が主流である。前の章でも扱っている最急降下法の一種である。\n",
    "\n",
    "\\begin{align*}\n",
    "w_{ij} \\gets w_{ij} - \\eta \\frac{\\partial L}{\\partial w_{ij}}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、ロス関数の返す値と、任意の重みパラメータ$w_{ij}$の依存関係についてもう少し詰めていくことにしよう。インデックス$j$に注目しつつ、以下の数量が重要である。\n",
    "\n",
    "\\begin{align*}\n",
    "L & = L((y_{l})_{l \\in O})\\\\\n",
    "y_{l} & = f_{l}(x_{l})\\\\\n",
    "x_{l} & = x_{l}(y_{j})\\\\\n",
    "y_{j} & = f_{j}(x_{j})\n",
    "\\end{align*}\n",
    "\n",
    "1行目：ロス関数$L$は出力層の関数であるとしている。2行目：出力層の出力値は言うまでもなくその層の入力に依存する。3行目：出力層の入力が$j$番目のユニットに依存するならば、$x_{l}$を$y_{j}$の関数と見るべきである。ここで「依存する」ことは直接的に接続しているときも、間接的に接続しているときもいえる。4行目：あとは$j$番目ユニットの出力がどのように$w_{ij}$に依存するかは、その入力値$x_{j}$と活性化関数$f_{j}$によって決まる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上のことを踏まえて、連鎖律を繰り返して適用することによって、便利な計算式を導出することができる。活性化関数はみな微分可能であるとして、以下のように展開できる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_{ij}} & = \\sum_{l \\in O} \\frac{\\partial L}{\\partial y_{l}} \\frac{\\partial y_{l}}{\\partial y_{j}} \\frac{\\partial y_{j}}{\\partial x_{j}} \\frac{\\partial x_{j}}{\\partial w_{ij}}\\\\\n",
    "& = y_{i} f_{j}^{\\prime}(x_{j}) \\sum_{l \\in O} \\frac{\\partial L}{\\partial y_{l}} \\frac{\\partial y_{l}}{\\partial y_{j}}\\\\\n",
    "& = y_{i} f_{j}^{\\prime}(x_{j}) \\frac{\\partial L}{\\partial y_{j}}.\n",
    "\\end{align*}\n",
    "\n",
    "便宜上、ここで$\\delta_{j} = f_{j}^{\\prime}(x_{j}) (\\partial L / \\partial y_{j})$と定義しておくと、綺麗に書き換えられる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_{ij}} = y_{i} \\delta_{j}.\n",
    "\\end{align*}\n",
    "\n",
    "このデルタに注目して、効率的な計算方法を導き出したのが、\"generalized delta rule\"と呼ばれる著名な手法である。RumelhartとMcClellandの両氏率いる研究グループによって、80年代なかばに知名度が急上昇した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一つの良い計算方法は、最上層のユニットのデルタを計算し、それを下層へと「伝播」させる。これは実際のところ、難しいことではない。先述の通り、出力層の$l \\in O$では、$\\delta_{l}$はロス関数の定義からすぐに計算できる。たとえば、2乗誤差の場合、以下のような形を取る。\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_{l} & = 2y_{l}(1 - y_{l})(y_{l} - t_{l}), & \\text{ logistic activations}\\\\\n",
    "\\delta_{l} & = 2(y_{l} - t_{l}), & \\text{ linear activations}\n",
    "\\end{align*}\n",
    "\n",
    "ほかのロス関数や活性化関数でもまったく同様に求めることができる。\n",
    "\n",
    "それから、出力層以外のユニット$l \\notin O$について重要なのは、$y_{l}$が直接接続しているユニットを追っていくことである。それ以外のユニットは$L$の値を考える上で関係がないので、無視しても良い。展開すると以下のようになる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial y_{l}} & = \\sum_{i:l \\to i} \\frac{\\partial L}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial y_{l}}\\\\\n",
    "& = \\sum_{i:l \\to i} \\frac{\\partial L}{\\partial y_{i}} f_{i}^{\\prime}(x_{i}) w_{li}.\n",
    "\\end{align*}\n",
    "\n",
    "前の定義を利用して整理しておくと、任意のデルタの再帰的な表現が得られる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_{l} = f_{l}^{\\prime}(x_{l}) \\sum_{i:l \\to i} \\delta_{i} w_{li}.\n",
    "\\end{align*}\n",
    "\n",
    "数式では明確であるが、言葉でも難しいことはない。任意の$\\delta_{l}$について、もし$l \\to i$を満たすあらゆるユニット$i$の$\\delta_{i}$をすでに計算しているのであれば、その単純な線形和で$\\delta_{l}$を手に入れることができる。上から始まり、段々と下りていく。これは典型的な「逆伝播法(back-propagation)」である。\n",
    "\n",
    "用語について： 入力層から入って、各ユニットの$y_{j}$を求める一連の作業をフォワードパス(*forward pass*)と呼ぶことが多い。これに対して、上記のデルタの計算など、重みの偏微分を上から下へと順序よく計算していくことをバックワードパス(*backward pass*)と呼ばれる。この用法は、90年代からすでに浸透しており、2018年現在でも広く使われている（例：後ほど見るChainerの文法に入っている）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、長々とニューラルネットワークの定式化と一つの最適化方法を見てきたが、これが一体どのように前節のリバースモード自動微分と関わるのだろうか。その答えは、先述の逆伝播法を賢く実装する方法がリバースモード自動微分そのものであり、後者のスペシャルケースとして捉えることができる。\n",
    "\n",
    "具体例を使って、先ほどのgeneralized delta ruleとの接点を見ていこう。下図のような単純なフィードフォワードニューラルネットワークを考える。\n",
    "\n",
    "<img src=\"img/nn_demo.png\"  alt=\"Hand-drawn NN example\" width=\"350\" height=\"245\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ユニットの添え字は絵に書いてある通り、$1,2,3,4,5,6$である。入力層を$u_{1},u_{2},u_{3}$と書く。残りは以下のように整理できる。\n",
    "\n",
    "\\begin{align*}\n",
    "u_{4} & = x_{4} = \\sum_{i \\to 4} w_{i4}u_{i}\\\\\n",
    "u_{5} & = x_{5} = \\sum_{i \\to 5} w_{i5}u_{i}\\\\\n",
    "u_{6} & = y_{4} = f_{4}(x_{4})\\\\\n",
    "u_{7} & = y_{5} = f_{5}(x_{5})\\\\\n",
    "u_{8} & = x_{6} = \\sum_{i \\to 5} w_{i6}u_{i}\\\\\n",
    "u_{9} & = y_{6} = f_{6}(x_{6})\n",
    "\\end{align*}\n",
    "\n",
    "例のデルタの計算は、自動微分を実行していくことで必然的に入手することになる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_{4} = \\frac{\\partial u_{9}}{\\partial u_{4}}, \\quad \\delta_{5} = \\frac{\\partial u_{9}}{\\partial u_{5}}, \\quad \\delta_{6} = \\frac{\\partial u_{9}}{\\partial u_{8}}\n",
    "\\end{align*}\n",
    "\n",
    "以上のように、リバースモード自動微分を正しく行うことで、デルタの再帰的な計算が自ずと行われることになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chainer_bg\"></a>\n",
    "## Chainerを使ってみよう\n",
    "\n",
    "ごく簡単な紹介ではあったが、自動微分とニューラルネットワークの基本的な考え方を見て、パラメータの最適化を効率的に行う道筋が見えてきている。ただし、多種多様なネットワークアーキテクチャをカバーしながら、これを実装する作業は多大なる労力を要する。幸い、この作業を代行してくれる優秀なエンジニアがたくさんいるので、機械学習の手法のプロトタイプを開発するにあたって、彼らが作ったライブラリを用いることが賢明であろう。言語をPythonに限定しても、オープンソースで使い勝手の良いパッケージが多数ある。\n",
    "\n",
    "そのなかから、我々が使うのは、__Chainer__という国産のディープラーニングAPIである[<a href=\"https://docs.chainer.org/en/stable/\">link</a>]。日本国内では有名だが、世界ではTensorFlow, PyTorch, Caffeなどの人気APIの陰に隠れている現状である。それでも、Chainerは大変よくできており、実に簡明な枠組みであり、高い自由度と充実した機能性を誇る。また、複数のGPUを同時に駆使する仕組みも整備されており、ソフトウェア工学のバックグラウンドが浅い人でもすぐに使えるはずである。\n",
    "\n",
    "技術的な特徴としては、開発者がいう\"Define-by-Run\"の仕組みである。実行前にネットワーク構造を固定するのではなく、実行時にフォワードパスを行いながら、動的にネットワーク構造を求めていく[<a href=\"https://docs.chainer.org/en/stable/guides/define_by_run.html\">link</a>]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerの導入については、ドキュメンテーションで推奨されているように[<a href=\"https://docs.chainer.org/en/stable/install.html\">link</a>]、`chainer`を`pip`で入手していることを前提に以下の作業を進めていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの造設にあたって、以下の点を念頭に置いて、作業に入る。\n",
    "\n",
    "- ありとあらゆるデータやパラメータ（普段、Numpyの配列に格納するもの）は、`Variable`オブジェクトとする[<a href=\"https://docs.chainer.org/en/stable/guides/variables.html\">link</a>]。\n",
    "\n",
    "- `Variable`オブジェクトはNumpyの配列とほぼ同じように扱うことができる[<a href=\"https://docs.chainer.org/en/stable/reference/generated/chainer.Variable.html#chainer.Variable\">link</a>].\n",
    "\n",
    "- `FunctionNode`オブジェクトはcomputational graphにおけるノードに相当する。正確にいえば、微分可能な関数を実装したものである[<a href=\"https://docs.chainer.org/en/stable/reference/generated/chainer.FunctionNode.html#chainer-functionnode\">link</a>]。\n",
    "\n",
    "- 各々の`FunctionNode`に対して、フォワードパスに相当する`forward()`と、バックワードパスに相当する`backward()`という2つのメソッドを必ず整備する。\n",
    "\n",
    "- ノードの入力を所与として、それを処理する`FunctionNode`は、`apply()`というメソッドによって実行される。実行時には、computational graphにノードが追加され、フォワードパスの計算結果を返すという働きである。\n",
    "\n",
    "- モデルを作る上で、基本単位となるのは`Link`というオブジェクトである[<a href=\"https://docs.chainer.org/en/stable/reference/generated/chainer.Link.html#chainer.Link\">link</a>]。ある`FunctionNode`に基づく`Link`では、扱っている多数の`Variable`のうち、どれが観測データに相当し、どれが最適化の対象となるパラメータなのかを指定する。一部の変数を`Parameter`オブジェクトとして「登録」する作業が重要である。あと、`Link`では、`FunctionNode`を`__call__()`するためのメソッドも備えている。\n",
    "\n",
    "- `Chain`と呼ばれるオブジェクトは、`Link`から構成されるもので、モデルの全体あるいはその一部を表わすために使われる（注：`Chain`も`Link`である）[<a href=\"https://docs.chainer.org/en/stable/reference/generated/chainer.Chain.html#chainer-chain\">link</a>]。段取りとしては、`Chain`を作成するときに、その構成要素に相当する\"child links\"を登録しておく。\n",
    "\n",
    "さて、機械学習にあたっての学習作業について考えよう。Chainerの標準搭載の技術を中心としたやり方だと、以下のような手順になる。\n",
    "\n",
    "- モデルのオブジェクト（ほぼ例外なく`Chain`のサブクラス）を、`setup()`というメソッドによって`Optimizer`オブジェクトに渡しておく。\n",
    "\n",
    "- 反復的には以下の計算を行う。\n",
    "\n",
    "  - 実数値`Variable`を計算し、例として`loss`と呼ぶ。\n",
    "  \n",
    "  - `loss.backward()`を実行して逆伝播法によって、出力層に相当する`loss`から、入力層の重みまで、すべての`FunctionNode`オブジェクトの偏微分を行う。\n",
    "  \n",
    "  - 勾配を手に入れた上で、`Optimizer`オブジェクトの`update()`を使って、登録した`Parameter`を更新する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chainer_exSimple\"></a>\n",
    "### 簡単な計算\n",
    "\n",
    "Chainerの基本的な働きを調べていくための簡単な事例をいくつか見ていこう。たとえば、以下の入出力の関係を考える。\n",
    "\n",
    "\\begin{align*}\n",
    "y = 2 x_{1}x_{2} + e^{x_{1}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import chainer as ch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients prior to calling y.backward():\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "Gradients after calling y.backward():\n",
      "[1.]\n",
      "[10.481689] ( answer = 10.481689 )\n",
      "[3.] ( answer = 3.0 )\n"
     ]
    }
   ],
   "source": [
    "touse_1 = 1.5\n",
    "touse_2 = 3.0\n",
    "\n",
    "x1 = ch.Variable(np.array([touse_1], dtype=np.float32))\n",
    "x2 = ch.Variable(np.array([touse_2], dtype=np.float32))\n",
    "y = 2*x1*x2 + math.exp(1)**x1\n",
    "\n",
    "# Note that all gradients start out empty.\n",
    "print(\"Gradients prior to calling y.backward():\")\n",
    "print(y.grad)\n",
    "print(x1.grad)\n",
    "print(x2.grad)\n",
    "\n",
    "# Now, compute the gradients of y.\n",
    "y.backward()\n",
    "print(\"\\nGradients after calling y.backward():\")\n",
    "print(y.grad)\n",
    "print(x1.grad, \"( answer =\",\n",
    "      np.float32(2*x2.data[0] + math.exp(1)**x1.data[0]),\n",
    "      \")\")\n",
    "print(x2.grad, \"( answer =\",\n",
    "      np.float32(2*x1.data[0] + 0),\n",
    "      \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちゃんと正解を出しているので、一安心である。この計算がどのように行われているか精密検査すべく、3つの層（$\\mathbf{u}_{0}, \\mathbf{u}_{1}, \\mathbf{u}_{2}$）に分ける。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{u}_{0} & = (u_{0,1}, u_{0,2}) = (x_{1}, x_{2})\\\\\n",
    "\\mathbf{u}_{1} & = (u_{1,1}, u_{1,2}) = (u_{0,1}u_{0,2}, e^{u_{0,1}})\\\\\n",
    "\\mathbf{u}_{2} & = (u_{2,1}) = (2u_{1,1} + u_{1,2}).\n",
    "\\end{align*}\n",
    "\n",
    "上記から明らかなように、$\\mathbf{u}_{0}$を入力として、$\\mathbf{u}_{0} \\mapsto \\mathbf{u}_{1} \\mapsto \\mathbf{u}_{2}$という2つの変換を経て、最終的な出力を得る。以下では、これらの変換をそれぞれ実装した`U1`および`U2`という`FunctionNode`サブクラスを掲げている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U1(ch.function_node.FunctionNode):\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        u0, = inputs\n",
    "        u1 = np.array([u0[0,0]*u0[0,1], math.exp(1)**u0[0,0]],\n",
    "                      dtype=np.float32)\n",
    "        u1 = u1.reshape((1,2))\n",
    "        self.retain_inputs((0,))\n",
    "        return (u1,)\n",
    "\n",
    "    def backward(self, indices, grad_outputs):\n",
    "        \n",
    "        u0, = self.get_retained_inputs()\n",
    "        u0 = u0.data\n",
    "        gy, = grad_outputs # defaults to shape (1,2).\n",
    "        print(\"U1 gy:\", gy.data)\n",
    "        \n",
    "        # Compute the Jacobian.\n",
    "        J = np.array([u0[0,1], u0[0,0], math.exp(1)**u0[0,0], 0.0],\n",
    "                     dtype=np.float32).reshape((2,2))\n",
    "        J = ch.Variable(J)\n",
    "        \n",
    "        # Compute partial derivatives of interest.\n",
    "        gu0 = gy @ J # gy.dot(J)\n",
    "        print(\"U1 backward():\", gu0.data)\n",
    "        return (gu0,)\n",
    "\n",
    "def fn_u1(u0):\n",
    "    args = (u0,)\n",
    "    y, = U1().apply(args)\n",
    "    return y\n",
    "\n",
    "\n",
    "class U2(ch.function_node.FunctionNode):\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        u1, = inputs\n",
    "        u2 = np.array([2*u1[0,0]+u1[0,1]])\n",
    "        self.retain_inputs((0,))\n",
    "        return (u2,)\n",
    "\n",
    "    def backward(self, indices, grad_outputs):\n",
    "        \n",
    "        u1, = self.get_retained_inputs()\n",
    "        gy, = grad_outputs # defaults to shape (1,).\n",
    "        gy = gy.reshape((1,1))\n",
    "        gy = ch.functions.cast(gy, np.float32)\n",
    "        print(\"U2 gy:\", gy.data)\n",
    "        \n",
    "        # Compute the Jacobian.\n",
    "        J = np.array([2.0, 1.0], dtype=np.float32).reshape((1,2))\n",
    "        J = ch.Variable(J)\n",
    "        \n",
    "        # Compute partial derivatives of interest.\n",
    "        gu1 = gy @ J # gy.dot(J)\n",
    "        print(\"U2 backward():\", gu1.data)\n",
    "        return (gu1,)\n",
    "    \n",
    "def fn_u2(u1):\n",
    "    args = (u1,)\n",
    "    y, = U2().apply(args)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードパスのときに使う`forward()`メソッドはすぐに整備できる。定義どおりに実装するだけである。\n",
    "\n",
    "バックワードパスのときに使う`backward()`に関しては、Chainerの規定をしっかりと理解しておく必要がある（下図を参照）。\n",
    "\n",
    "<img src=\"img/chainer_backward_out.png\" alt=\"Chainer documentation on backward pass\" width=\"400\" height=\"250\" />\n",
    "\n",
    "要するに、各層の入出力の偏微分を整列させたヤコビ行列を計算し、`grad_outputs`から渡された係数をかけて、出力のインデックスについて足し合わせていく。上記の`U1`で計算している`J`の寸法は$(2 \\times 2)$で、次の形を取る。\n",
    "\n",
    "\\begin{align*}\n",
    "\\left[ \\frac{\\partial u_{1,i}}{\\partial u_{0,j}} \\right]_{i,j}, \\quad i = 1,2 \\quad j = 1,2.\n",
    "\\end{align*}\n",
    "\n",
    "`U2`で計算している`J`の寸法は$(1 \\times 2)$で、以下の形を取る。\n",
    "\n",
    "\\begin{align*}\n",
    "\\left[ \\frac{\\partial u_{2,i}}{\\partial u_{1,j}} \\right]_{i,j}, \\quad i = 1 \\quad j = 1,2.\n",
    "\\end{align*}\n",
    "\n",
    "リバースモード自動微分の話を思い出すと、次の再帰的な計算が重要であった。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{U}_{i} = \\bar{U}_{i+1} \\widetilde{J}_{i}\n",
    "\\end{align*}\n",
    "\n",
    "仮に層の数が$K$であるとして、これらの行列は以下のとおりである。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{U}_{i} & = \\left[ \\frac{\\partial y_{j}}{\\partial u_{i,k}} \\right]_{j,k} \\quad (d_{K} \\times d_{i}), \\quad i = 0,1,\\ldots,K \\\\\n",
    "\\widetilde{J}_{i} & = \\left[ \\frac{\\partial u_{i+1,j}}{\\partial u_{i,k}} \\right]_{j,k} \\quad (d_{i+1} \\times d_{i}), \\quad i=0,1,\\ldots,K-1.\n",
    "\\end{align*}\n",
    "\n",
    "具体例に戻ると、$K=2$で、次元の数は$d_{0}=2, d_{1}=2, d_{2}=1$となる。以下のコードでは、`backward()`を我々の手作りノードに対して実行することで、その結果が元のコードの結果と完全に一致することがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of forward passes:\n",
      "y: 13.481689\n",
      "_y: 13.481688976287842\n",
      "\n",
      "Content of our backward() passes:\n",
      "U2 gy: [[1.]]\n",
      "U2 backward(): [[2. 1.]]\n",
      "U1 gy: [[2. 1.]]\n",
      "U1 backward(): [[10.481689  3.      ]]\n",
      "\n",
      "Gradient of (x1,x2): (10.481689, 3.0)\n",
      "Gradient of u0: (10.481689, 3.0)\n"
     ]
    }
   ],
   "source": [
    "touse_1 = 1.5\n",
    "touse_2 = 3.0\n",
    "\n",
    "# Original code.\n",
    "x1 = ch.Variable(np.array([touse_1], dtype=np.float32))\n",
    "x2 = ch.Variable(np.array([touse_2], dtype=np.float32))\n",
    "y = 2*x1*x2 + math.exp(1)**x1\n",
    "y = y.reshape((1,1))\n",
    "\n",
    "# Our custom reformulation.\n",
    "_u0 = np.array([touse_1, touse_2], dtype=np.float32).reshape((1,2))\n",
    "_u0 = ch.Variable(_u0)\n",
    "_u1 = fn_u1(u0=_u0)\n",
    "_y = fn_u2(u1=_u1)\n",
    "_y = _y.reshape((1,1))\n",
    "\n",
    "print(\"Output of forward passes:\")\n",
    "print(\"y:\", *y.data[0])\n",
    "print(\"_y:\", *_y.data[0])\n",
    "\n",
    "y.backward()\n",
    "print(\"\\nContent of our backward() passes:\")\n",
    "_y.backward()\n",
    "\n",
    "print(\"\\nGradient of (x1,x2):\", (x1.grad[0], x2.grad[0]))\n",
    "print(\"Gradient of u0:\", tuple(*_u0.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の実行結果から明らかなように、2層目の`backward()`の結果が、下の1層目に`grad_outputs`という形で渡されている。期待どおりの計算である。\n",
    "\n",
    "__重要：__ 一般的な自動微分の枠組みでは、究極の計算目的である$\\bar{U}_{0}$が($d_{K} \\times d_{0}$)という形をとっても、何も問題はない。しかし、Chainerでは、必ず$d_{K}=1$と制限されていることに注意が必要である。これが事実であることは、明らかである。その理由は、Chainerのドキュメンテーションより、関数$f: \\mathbb{R}^{a} \\to \\mathbb{R}^{b}$のバックワードパス計算である`backward()`を実装する際、その出力が$\\mathbb{R}^{a}$でなければならないからである。\n",
    "\n",
    "しかし、数学的には、これは大丈夫であろうか。何を意味するものなのか。さらに検証していくために、先ほどの具体例に戻るが、今度はbackpropの対象を最終層の$\\mathbf{u}_{2} \\in \\mathbb{R}$ではなく、$\\mathbf{u}_{1} \\in \\mathbb{R}^{2}$とする（なお、$\\mathbf{u}_{0}$について微分を求めていることは変わらない）。そのためには、`grad_outputs`を、勾配を初期化する（`grad`を設定する）ことで、自分で渡す必要がある（以下のコードを参照）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U1 gy: [[1. 1.]]\n",
      "U1 backward(): [[7.481689 1.5     ]]\n",
      "_u0.grad = (7.481689, 1.5)\n"
     ]
    }
   ],
   "source": [
    "_u0 = np.array([touse_1, touse_2], dtype=np.float32).reshape((1,2))\n",
    "_u0 = ch.Variable(_u0)\n",
    "_u1, = fn_u1(u0=_u0)\n",
    "_u1.grad = np.ones(_u1.shape, dtype=np.float32)\n",
    "_u1.backward()\n",
    "print(\"_u0.grad =\", tuple(*_u0.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すぐにわかるのは、$(1.0, 1.0)$に初期化したとき、計算結果が前と異なることである。その理由は、最後の変換が反映されていないからである。さて、今の例では、逆伝播法の対象となる関数の出力が多次元であることもあって、`backward()`の実行結果がヤコビ行列$\\partial \\mathbf{u}_{1} / \\partial \\mathbf{u}_{0}$となることを予想する読者がいるかもしれない。しかし、先述の理由により、これはあり得ないことである。勾配ベクトルしか保存されない。\n",
    "\n",
    "そうなると、数学的に、Chainerの中で一体何の関数が微分されているのだろうか。その答えは簡単で、以下のようなダミー関数が微分されているのである。\n",
    "\n",
    "\\begin{align*}\n",
    "f(\\mathbf{u}_{K}) = a_{1} u_{K,1} + \\cdots + a_{K} u_{K,d_{K}}\n",
    "\\end{align*}\n",
    "\n",
    "ここでの$\\mathbf{u}_{K}$は我々のモデルの最終層の出力に相当し、係数$(a_{1},\\ldots,a_{K})$は逆伝播に際して最初に使われる`grad_outputs`に相当する。上の例では、\n",
    "\n",
    "```\n",
    "_u1.grad = np.ones(_u1.shape, dtype=np.float32)\n",
    "```\n",
    "\n",
    "という初期化をしたのだが、これは上記のダミー関数で、$a_{1}=\\cdots=a_{K}=1$と置いたことにほかならない。\n",
    "\n",
    "上を要約すると、最終層の出力がスカラー値であれば、$d_{K}=1$なのですべてが直感通りである。一方の$d_{K} > 1$の場合、`grad`をどのように初期化するかによって、バックエンドに隠れているダミー関数の係数が変わるため、計算結果も変わってくる。ちなみに、$d_{K}=1$の場合は、わざわざ初期化する必要はなく、自動的に`grad`が1.0に初期化されるようになっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__練習問題__\n",
    "\n",
    "0. 上の具体例で、2層目を自動微分の対象とした場合、`_u1.grad`を全部1.0にした結果、`_u1.backward()`を実行した結果が、もともとの`_u2.backward()`の実行結果と異なるものであった。どのように初期化すれば、前者が後者に一致するか。\n",
    "\n",
    "0. 上記の具体例を拡張すべく、たとえば3つか4つほどの層を持つように新しい演算を追加すること。上と同様に、自分で実装し、Chainerの計算結果が我々の数学的な予想と一致するかどうか確認すること。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chainer_linreg\"></a>\n",
    "### 線形回帰での最小二乗法を再現\n",
    "\n",
    "練習の対象として、<a href=\"FrameworkIntro.ipynb\">前の章</a>では下記の学習機を取り上げた。\n",
    "\n",
    "> - データ：入力ベクトルと実数値の応答$x \\in \\mathbb{R}^{d}$, $y \\in \\mathbb{R}$.データセットの全体は$\\{(x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\}$とする。\n",
    "> \n",
    "> - モデル：二乗誤差を用いた線形回帰モデル。つまり、$y = \\langle w^{\\ast}, x\\rangle + \\varepsilon$と仮定している。ロス関数は$L(w;x,y) = (y - \\langle w, x\\rangle)^{2}/2$で、勾配が$\\nabla L(w;x,y) = -(y-\\langle w, x\\rangle)x$である。\n",
    "> \n",
    "> - アルゴリズム：最急降下法を使って、固定したステップサイズ$\\alpha > 0$を用いる。数式で表わすと、$z_{i}=(x_{i},y_{i})$として以下の通りである。\n",
    "> \n",
    "> \\begin{align*}\n",
    "w_{(t+1)} \\gets w_{(t)} - \\alpha \\frac{1}{n} \\sum_{i=1}^{n}\\nabla L(w_{(t)}; z_{i}).\n",
    "\\end{align*}\n",
    "\n",
    "前の章で叩きだした各種のクラスなど、上記を実装するのに必要なものは全部`algorithms.py`, `models.py`, `dataclass.py`, and `helpers.py`に収めてある。ここでは、自作の学習機を、ChainerのAPIによって完全に再現することである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは必要なモジュールを`import`して、いわゆる\"fully connected linear layer\"を実装した`FunctionNode`を整備する（活性化関数を介さず、入力の線形和を取るだけ）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import chainer as ch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import algorithms\n",
    "import models\n",
    "import dataclass\n",
    "import helpers as hlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunctionNode sub-class, for implementing a fully connected linear layer.\n",
    "\n",
    "class LinearFunction(ch.function_node.FunctionNode):\n",
    "    '''\n",
    "    FunctionNode object, defined on Variable objects,\n",
    "    which is the basis for a linear transformation to\n",
    "    be wrapped up as a Link object.\n",
    "    \n",
    "    Take d-dimensional inputs x, and using array W of\n",
    "    shape (k,d), map this input to k outputs. That is,\n",
    "    we have a fully-connected layer with d input units\n",
    "    and k output units.\n",
    "    '''\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Forward computation for both CPU and GPU.\n",
    "        '''\n",
    "        \n",
    "        # Unpack the tuple of inputs.\n",
    "        if len(inputs) == 3:\n",
    "            x, W, b = inputs\n",
    "        else:\n",
    "            (x, W), b = inputs, None\n",
    "\n",
    "        y = x.dot(W.T).astype(x.dtype, copy=False)\n",
    "        \n",
    "        # Add a bias term, if relevant.\n",
    "        if b is not None:\n",
    "            y += b\n",
    "            \n",
    "        # Since backward() depends only on x and W,\n",
    "        # we need only retain these two.\n",
    "        self.retain_inputs((0,1))\n",
    "        \n",
    "        # Must return the output as a tuple.\n",
    "        return (y,)\n",
    "\n",
    "    def backward(self, indices, grad_outputs):\n",
    "        '''\n",
    "        General-purpose computation for both CPU/GPU.\n",
    "        '''\n",
    "        \n",
    "        x, W = self.get_retained_inputs()\n",
    "        gy, = grad_outputs # written as gamma in their docs.\n",
    "        \n",
    "        # Says that backward() must return a tuple, but\n",
    "        # looking at their source code for linear.py, it\n",
    "        # seems like lists are fine.\n",
    "        out = []\n",
    "        if 0 in indices:\n",
    "            gx = gy @ W # gy.dot(W)\n",
    "            out.append(ch.functions.cast(gx, x.dtype))\n",
    "        if 1 in indices:\n",
    "            gW = gy.T @ x # gy.T.dot(x)\n",
    "            out.append(ch.functions.cast(gW, W.dtype))\n",
    "        if 2 in indices:\n",
    "            # Summing here is simple: for n observations,\n",
    "            # gy has shape (n,k), where k is the number of\n",
    "            # layer outputs. Summing over axis=0 is summing\n",
    "            # over OBSERVATIONS, not over outputs.\n",
    "            gb = ch.functions.sum(gy, axis=0)\n",
    "            \n",
    "        # Return just the relevant gradients we appended.\n",
    "        return out\n",
    "\n",
    "\n",
    "def linear(x, W, b):\n",
    "    '''\n",
    "    A nice thin wrapper for our linear FunctionNode on\n",
    "    Variable objects.\n",
    "    '''\n",
    "    \n",
    "    if b is None:\n",
    "        args = (x, W)\n",
    "    else:\n",
    "        args = (x, W, b)\n",
    "    \n",
    "    # Don't forget to unpack from the tuple.\n",
    "    y, = LinearFunction().apply(args)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FunctionNode`の要点：\n",
    "\n",
    "- `forward()`はNumpyの配列だけで定義している（`Variable`でも良いが）。\n",
    "\n",
    "- `backward()`はサンプルの上で足しあわせていることに注意が必要。\n",
    "\n",
    "- さらに、`backward()`となると、`Variable`オブジェクトを前提とした実装にしなければならない（特別な演算記号`@`などがあるおは、そのためである）。\n",
    "\n",
    "次は、このノードに基づく`Link`オブジェクトを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link object for our linear FunctionNode.\n",
    "\n",
    "class Linear(ch.Link):\n",
    "    '''\n",
    "    A Link class for our linear transformation, implemented\n",
    "    in the LinearFunction class.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 in_size, out_size,\n",
    "                 init_W=None, init_b=None,\n",
    "                 init_delta=None,\n",
    "                 nobias=False):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        # Here we initialize and \"register\" the parameters\n",
    "        # of interest. This is critical because when we\n",
    "        # call __call__(x) and apply the underlying affine\n",
    "        # transformations to input x (both forward pass and\n",
    "        # backward pass), the optimization algorithms knows\n",
    "        # that we want to optimize W and maybe b, but not x.\n",
    "\n",
    "        with self.init_scope():\n",
    "            \n",
    "            # If provided an ndarray, use it.\n",
    "            if init_W is not None:\n",
    "                self.W = ch.Parameter(initializer=np.copy(init_W))\n",
    "            \n",
    "            # Else, use a built-in initializer.\n",
    "            else:\n",
    "                W_initializer = ch.initializers.Uniform(scale=init_delta,\n",
    "                                                        dtype=np.float32)\n",
    "                self.W = ch.Parameter(initializer=W_initializer,\n",
    "                                      shape=(out_size, in_size))\n",
    "            \n",
    "            if nobias:\n",
    "                self.b = None\n",
    "            else:\n",
    "                if init_b is not None:\n",
    "                    self.b = ch.Parameter(initializer=np.copy(init_b))\n",
    "                else:\n",
    "                    self.b = ch.Parameter(initializer=0,\n",
    "                                          shape=(out_size,))\n",
    "                \n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        This method actually applies the linear layer to\n",
    "        inputs x.\n",
    "        '''\n",
    "        return linear(x, self.W, self.b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この`Link`の定義でもっとも重要なのは、学習すべきパラメータを登録していることである。その段取りとしては、`init_scope()`なるコンテキストマネジャーを使って、指定したパラメータを属性(attributes)として格納しておく。この`Link`は先ほどの`LinearFunction`に基づくものであり、`__call__()`によって呼び出される。\n",
    "\n",
    "上記で区別しているのは、データの`x`と、学習すべきパラメータの`W`および`b`である。いずれも計算する必要があるのだが、最適化の対象となるのは後者のみであるから、明確に区別することがきわめて重要である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、`Chain`オブジェクトを先ほどの`Link`から造設していく。`Chain`の定義が`Link`の定義に酷似しているのは、`Chain`自体が`Link`でもあるからである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain object, composed of Link objects. This is a proper model in the\n",
    "# sense that it can be fed to optimizers.\n",
    "\n",
    "class Chain_LinReg(ch.Chain):\n",
    "    '''\n",
    "    Perhaps the simplest possible model, a\n",
    "    feed-forward neural network without any\n",
    "    hidden layers. Just one, fully-connected\n",
    "    linear layer, aka a classical linear\n",
    "    regression model, with arbitrary number\n",
    "    of outputs.\n",
    "    \n",
    "    out_l0: number of outputs from layer 0.\n",
    "    out_l1: number of outputs from layer 1.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 out_l0,\n",
    "                 out_l1,\n",
    "                 init_W=None,\n",
    "                 init_b=None,\n",
    "                 init_delta=1.0,\n",
    "                 nobias=False):\n",
    "        super(Chain_LinReg, self).__init__()\n",
    "        \n",
    "        with self.init_scope():\n",
    "            self.l1 = Linear(in_size=out_l0,\n",
    "                             out_size=out_l1,\n",
    "                             init_W=init_W,\n",
    "                             init_b=init_b,\n",
    "                             init_delta=init_delta,\n",
    "                             nobias=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.l1(x) # parameters are managed by Links.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Link`オブジェクトがパラメータを登録するのと同様に、`Chain`オブジェクトは\"child links\"を登録し、属性として格納する（ここもやはり`init_scope()`なるコンテキストマネジャーを使う）。\n",
    "\n",
    "我々が扱っている今回の具体例では、モデルが至ってシンプルなので、長さ`out_l0`のベクトルを入力として受け取り、たった1回の行列の掛け算を`Linear`で実行し、出力する。その実行結果の長さが`out_l1`である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我々としてもっとも重要なのは、勾配降下法などに際して、Chainerが予想通りに偏微分を計算してくれているかどうかである。丁寧にこれを検証するために、同一のデータセットに対して、自作の学習機とChainerによって作った学習機を両方試してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment setup.\n",
    "\n",
    "# Data-related.\n",
    "data = dataclass.DataSet() # Initialize one data object; will be re-populated at each trial.\n",
    "n = 500 # sample size\n",
    "d = 2 # number of parameters\n",
    "init_delta = 5.0 # controls support of random initialization\n",
    "num_trials = 250 # number of independent random trials to conduct\n",
    "cov_X = np.eye(d) # covariance matrix of the inputs.\n",
    "\n",
    "w_star = np.ones(d).reshape((d,1)) # vector specifying true model\n",
    "\n",
    "# Algorithm-related.\n",
    "m_idx_todo = [0,1] # let's us manually pick and choose which methods to evaluate.\n",
    "t_max = 30 # termination condition; maximum number of iterations.\n",
    "thres = -1.0 # termination condition; if negative, runs for max iterations.\n",
    "\n",
    "def alpha_fixed(t, val): # step-size callback function.\n",
    "    return val\n",
    "def make_step(u):\n",
    "    def mystep(t, model=None, data=None, newdir=None):\n",
    "        return alpha_fixed(t=t, val=u)\n",
    "    return mystep\n",
    "alphaval = 0.25\n",
    "\n",
    "# Clerical.\n",
    "mth_names = [\"gd\", \"gd-ch\"]\n",
    "num_mths = len(mth_names)\n",
    "mth_colours = [\"black\", \"blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make choice of additive noise distribution (un-comment your choice).\n",
    "#paras = {\"name\": \"norm\", \"shift\": 0.0, \"scale\": 20.0}\n",
    "paras = {\"name\": \"lnorm\", \"meanlog\": 0.0, \"sdlog\": 1.75}\n",
    "\n",
    "# Put together risk function.\n",
    "def risk(w):\n",
    "    mean_noise, var_noise = hlp.noise_risk(paras=paras)\n",
    "    return hlp.riskMaker(w=w, A=cov_X, b=math.sqrt(var_noise), w_star=w_star)\n",
    "risk_star = risk(w=w_star) # optimal risk value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the algorithms.\n",
    "\n",
    "# Prepare storage for performance metrics.\n",
    "riskvals = np.zeros((num_trials,t_max,num_mths), dtype=np.float32)\n",
    "loss_tr = np.zeros((num_trials,t_max,num_mths), dtype=np.float32)\n",
    "truedist = np.zeros((num_trials,t_max,num_mths), dtype=np.float32)\n",
    "\n",
    "# Loop over trials.\n",
    "for tri in range(num_trials):\n",
    "    \n",
    "    # Generate new data (with *centered* noise).\n",
    "    X = np.random.normal(loc=0.0, scale=1.0, size=(n,d))\n",
    "    noise = hlp.noise_data(n=n, paras=paras)\n",
    "    y = np.dot(X, w_star) + noise\n",
    "    data.init_tr(X=X, y=y)\n",
    "    \n",
    "    # Data for Chainer model.\n",
    "    Z = ch.datasets.TupleDataset(np.float32(X),\n",
    "                                 np.float32(y))\n",
    "    \n",
    "    # Initial weight settings.\n",
    "    w_init = w_star + np.random.uniform(low=-init_delta, high=init_delta, size=d).reshape((d,1))\n",
    "    w_init = np.float32(w_init)\n",
    "    \n",
    "    # Initialize models (hand-built).\n",
    "    mod_learner = models.LinearL2(data=data)\n",
    "    risk_star = risk(w=w_star) # optimal risk value.\n",
    "    loss_star = np.mean(mod_learner.l_tr(w=w_star, data=data))\n",
    "    \n",
    "    # Initialize models (Chainer-based).\n",
    "    mod_chainer = Chain_LinReg(out_l0=d,\n",
    "                               out_l1=1,\n",
    "                               init_W=w_init.T,\n",
    "                               init_b=None,\n",
    "                               init_delta=init_delta,\n",
    "                               nobias=True)\n",
    "    \n",
    "    # Initialize algorithms (hand-built).\n",
    "    al_gd = algorithms.Algo_GD(w_init=w_init,\n",
    "                               step=make_step(alphaval),\n",
    "                               t_max=t_max,\n",
    "                               thres=thres,\n",
    "                               store=True,\n",
    "                               lamreg=None)\n",
    "    \n",
    "    # Initialize algorithms (Chainer-based).\n",
    "    opt_chainer = ch.optimizers.SGD(lr=alphaval)\n",
    "    opt_chainer.setup(mod_chainer) # pass model!\n",
    "\n",
    "    \n",
    "    # Run all algorithms and save their performance.\n",
    "    \n",
    "    ## ERM-GD.\n",
    "    mthidx = 0\n",
    "    if mthidx in m_idx_todo:        \n",
    "        idx = 0\n",
    "        for mystep in al_gd:\n",
    "            al_gd.update(model=mod_learner, data=data)\n",
    "            # Record performance\n",
    "            loss_tr[tri,idx,mthidx] = np.mean(mod_learner.l_tr(w=al_gd.w, data=data))-loss_star\n",
    "            riskvals[tri,idx,mthidx] = risk(w=al_gd.w)-risk_star\n",
    "            truedist[tri,idx,mthidx] = np.linalg.norm(w_star-al_gd.w)-0\n",
    "            idx += 1\n",
    "        \n",
    "        \n",
    "    ## Replication of ERM using Chainer.\n",
    "    mthidx = 1\n",
    "    if mthidx in m_idx_todo:\n",
    "        idx = 0\n",
    "        iter_train = ch.iterators.SerialIterator(dataset=Z,\n",
    "                                                 batch_size=n, # thus SGD=GD; deterministic.\n",
    "                                                 repeat=True,\n",
    "                                                 shuffle=False)\n",
    "        while iter_train.epoch < t_max:\n",
    "            \n",
    "            # Get our mini-batch.\n",
    "            Z_batch = iter_train.next()\n",
    "            X_batch, y_batch = ch.dataset.concat_examples(Z_batch)\n",
    "\n",
    "            # Predictions.\n",
    "            prediction_tr = mod_chainer(X_batch)\n",
    "\n",
    "            # Loss computations (will feed the grad computations).\n",
    "            loss = ch.functions.mean_squared_error(prediction_tr, y_batch) / 2.0\n",
    "            #old_loss = np.mean(mod_learner.l_tr(w=mod_chainer.l1.W.data.T, data=data))\n",
    "            #print(\"old versus new:\")\n",
    "            #print(loss, old_loss)\n",
    "            loss_star = np.mean(mod_learner.l_tr(w=w_star, data=data))\n",
    "            \n",
    "            # Gradient computations.\n",
    "            mod_chainer.cleargrads()\n",
    "            loss.grad = np.ones(loss.data.shape, dtype=np.float32)\n",
    "            loss.backward()\n",
    "\n",
    "            # Parameter updates.\n",
    "            opt_chainer.update()\n",
    "\n",
    "            # Record performance\n",
    "            loss_tr[tri,idx,mthidx] = np.mean(mod_learner.l_tr(w=mod_chainer.l1.W.data.T, data=data))-loss_star\n",
    "            riskvals[tri,idx,mthidx] = risk(w=mod_chainer.l1.W.data.T)-risk_star\n",
    "            truedist[tri,idx,mthidx] = np.linalg.norm(w_star-mod_chainer.l1.W.data.T)-0\n",
    "            idx += 1\n",
    "\n",
    "\n",
    "# Finally, take statistics of the performance metrics over all trials.\n",
    "ave_loss_tr = np.mean(loss_tr, axis=0)\n",
    "ave_riskvals = np.mean(riskvals, axis=0)\n",
    "ave_truedist = np.mean(truedist, axis=0)\n",
    "sd_loss_tr = np.std(loss_tr, axis=0)\n",
    "sd_riskvals = np.std(riskvals, axis=0)\n",
    "sd_truedist = np.std(truedist, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要点：\n",
    "\n",
    "- `loss.grads`を手動で初期化していないことは、微分の対象となるものがスカラーだからである（先述）。\n",
    "\n",
    "- 反復的に更新しているが、複数ステージにわたって勾配情報が加算されていくと誤った計算結果につながるため、必ず`cleargrads()`を実行すること。これは`mod_chainer`と呼んでいる`Chain`のメソッドである。\n",
    "\n",
    "- 今回使っている`Optimizer`オブジェクト（`opt_chainer`）がパラメータの更新をChainer側でさばいてくれている。学習したい`Parameter`はすべて`mod_chainer`で登録していること、また`mod_chainer`が`opt_chainer`に渡されていることなどを踏まえると、最適化担当はしかるべき情報をもらっていることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualization of performance.\n",
    "\n",
    "tvals = np.arange(t_max)+1 # better to start from the first update.\n",
    "\n",
    "# Average over trials.\n",
    "myfig = plt.figure(figsize=(15,5))\n",
    "\n",
    "ax_loss_tr = myfig.add_subplot(1, 3, 1)\n",
    "for m in m_idx_todo:\n",
    "    vals = ave_loss_tr[:,m]\n",
    "    err = sd_loss_tr[:,m]\n",
    "    ax_loss_tr.plot(tvals, vals, color=mth_colours[m], label=mth_names[m])\n",
    "    #ax_loss_tr.errorbar(tvals, vals, yerr=err, fmt='-o', col=mth_colours[m], label=mth_names[m])\n",
    "    ax_loss_tr.legend(loc=1,ncol=1)\n",
    "    plt.axhline(y=0.0, color=\"black\")\n",
    "    plt.title(\"Excess empirical risk\")\n",
    "\n",
    "ax_riskvals = myfig.add_subplot(1, 3, 2)\n",
    "for m in m_idx_todo:\n",
    "    vals = ave_riskvals[:,m]\n",
    "    err = sd_riskvals[:,m]\n",
    "    err_log = err / vals\n",
    "    ax_riskvals.semilogy(tvals, vals, \"-o\", color=mth_colours[m], label=mth_names[m])\n",
    "    ax_riskvals.legend(loc=1,ncol=1)\n",
    "    plt.axhline(y=0.0, color=\"black\")\n",
    "    plt.title(\"Excess risk\")\n",
    "\n",
    "ax_variation = myfig.add_subplot(1, 3, 3)\n",
    "for m in m_idx_todo:\n",
    "    vals = sd_riskvals[:,m]\n",
    "    ax_variation.plot(tvals, vals, \"-o\", color=mth_colours[m], label=mth_names[m])\n",
    "    ax_variation.legend(loc=1,ncol=1)\n",
    "    plt.axhline(y=0.0, color=\"black\")\n",
    "    plt.title(\"Variation of risk across samples\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__練習問題__\n",
    "\n",
    "0. 上記の具体例では、終了条件として`iter_train.epoch`を使っている。Chainerでは、epochが何をしているか説明すること。ミニバッチの大きさを$n$より小さな整数にした場合、振る舞いがどのように変わるか（エラーメッセージは出るが、それもなぜか）説明すること。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chainer_exNonLin\"></a>\n",
    "### 任意の活性化関数を用いた非線形モデル\n",
    "\n",
    "前の節に続き、線形モデルから単純な非線形モデルへと拡張していく。入力$\\mathbf{x} \\in \\mathbb{R}^{d_{0}}$を所与として、$\\mathbf{y} = (y_{1},\\ldots,y_{d_{1}})$への変換を次のように表記する。\n",
    "\n",
    "- 線形： $y_{j} = \\mathbf{w}_{j}^{T}\\mathbf{x} + b_{j}$\n",
    "\n",
    "- 非線形： $y_{j} = \\phi(\\mathbf{w}_{j}^{T}\\mathbf{x} + b_{j})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらの変換を実装した`FunctionNode`の`backward()`メソッドでは、寸法$(d_{1} \\times d_{0})$のヤコビ行列$\\partial \\mathbf{y} / \\partial \\mathbf{w}$を計算し、寸法($1 \\times d_{1}$)の`grad_outputs`との行列掛け算をする必要がある。ヤコビ行列自体はすぐに計算できる。線形の場合：\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial y_{i}}{\\partial w_{j,k}} =\n",
    "\\begin{cases}\n",
    "0, & i \\neq j\\\\\n",
    "x_{k}, & i = j\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "非線形の場合も同様に：\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial y_{i}}{\\partial w_{j,k}} =\n",
    "\\begin{cases}\n",
    "0, & i \\neq j\\\\\n",
    "\\phi^{\\prime}(\\mathbf{w}_{j}^{T}\\mathbf{x} + b_{j}) \\, x_{k}, & i = j.\n",
    "\\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、`grad_outputs`から渡されたものを$(\\gamma_{1},\\ldots,\\gamma_{d_{1}})$と書くことにすると、我々の`backward()`が以下の計算結果を返さなければならない。\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{d_{1}} \\gamma_{i} \\frac{\\partial y_{i}}{\\partial w_{j,k}} = \\gamma_{j} \\frac{\\partial y_{j}}{\\partial w_{j,k}} = \\gamma_{j} \\, \\phi^{\\prime}(\\mathbf{w}_{j}^{T}\\mathbf{x} + b_{j}) \\, x_{k}\n",
    "\\end{align*}\n",
    "\n",
    "インデックスの取る値は$j=1,\\ldots,d_{1}$と$k=1,\\ldots,d_{0}$である。したがって、出力するのは、容易に求められる以下のベクトルである。\n",
    "\n",
    "\\begin{align*}\n",
    "\\left( \\gamma_{1} \\, \\phi^{\\prime}(\\mathbf{w}_{1}^{T}\\mathbf{x} + b_{1}), \\ldots, \\gamma_{d_{1}} \\, \\phi^{\\prime}(\\mathbf{w}_{d_{1}}^{T}\\mathbf{x} + b_{d_{1}}) \\right).\n",
    "\\end{align*}\n",
    "\n",
    "以下で非線形モデルを実装するとき、このベクトルは`gy_nl`に相当する(以前として$1 \\times d_{1}$の二次元配列)。$\\mathbf{b}$や$\\mathbf{x}$についての偏微分もまったく同様に線形から非線形へと拡張することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import chainer as ch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearFunction(ch.function_node.FunctionNode):\n",
    "    '''\n",
    "    The Function object defined on Variable objects\n",
    "    that is the basis for a linear transformation to\n",
    "    be wrapped up as a Link object. The basic idea is\n",
    "    that we have d-dimensional inputs x, and the array\n",
    "    called \"W\" is of the shape (k, d). Thus the output\n",
    "    of x.dot(W.T) is that of precisely k \"units\".\n",
    "    '''\n",
    "    \n",
    "    def f_nonlin(self, u):\n",
    "        '''\n",
    "        Non-linear activation function.\n",
    "        Here: vectorized logistic function.\n",
    "        '''\n",
    "        return 1 / (1+np.exp(-u))\n",
    "    \n",
    "    def df_nonlin(self, u):\n",
    "        '''\n",
    "        First-order derivative of non-linear\n",
    "        activation function.\n",
    "        Here: vectorized logistic function deriv.\n",
    "        '''\n",
    "        y = self.f_nonlin(u=u)\n",
    "        return y * (1-y)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Forward computation for both CPU and GPU.\n",
    "        '''\n",
    "        \n",
    "        # Unpack the tuple of inputs.\n",
    "        if len(inputs) == 3:\n",
    "            x, W, b = inputs\n",
    "        else:\n",
    "            (x, W), b = inputs, None\n",
    "\n",
    "        y = x.dot(W.T).astype(x.dtype, copy=False)\n",
    "        \n",
    "        # Add a bias term, if relevant.\n",
    "        if b is not None:\n",
    "            y += b\n",
    "            self.retain_inputs((0,1,2))\n",
    "        else:\n",
    "            self.retain_inputs((0,1))\n",
    "            \n",
    "        # Finally the non-linear activation.\n",
    "        y = self.f_nonlin(y)\n",
    "        \n",
    "        # Must return the output as a tuple.\n",
    "        return (y,)\n",
    "    \n",
    "\n",
    "    def backward(self, indices, grad_outputs):\n",
    "        '''\n",
    "        Backward computation for both CPU and GPU.\n",
    "        '''\n",
    "        \n",
    "        if len(indices)==3:\n",
    "            x, W, b = self.get_retained_inputs()\n",
    "        else:\n",
    "            (x, W), b = self.get_retained_inputs(), ch.Variable(None)\n",
    "        \n",
    "        y = x.data.dot(W.data.T).astype(x.dtype, copy=False) # (n,k)\n",
    "        #y, = self.forward(inputs=(x.data, W.data, b.data)) # (n,k). Needed for grad comps.\n",
    "        gy, = grad_outputs # written as gamma in their docs.\n",
    "        gy_nl = ch.Variable(self.df_nonlin(y)) * gy # to account for non-linearity.\n",
    "        \n",
    "        # Says that backward() must return a tuple, but\n",
    "        # looking at their source code for linear.py, it\n",
    "        # seems like lists are fine.\n",
    "        out = []\n",
    "        if 0 in indices:\n",
    "            gx = gy_nl @ W # gy_nl.dot(W)\n",
    "            out.append(ch.functions.cast(gx, x.dtype))\n",
    "        if 1 in indices:\n",
    "            gW = gy_nl.T @ x # gy_nl.T.dot(x)\n",
    "            out.append(ch.functions.cast(gW, W.dtype))\n",
    "        if 2 in indices:\n",
    "            # Summing here is simple: for n observations,\n",
    "            # gy has shape (n,k), where k is the number of\n",
    "            # layer outputs. Summing over axis=0 is summing\n",
    "            # over OBSERVATIONS, not over outputs.\n",
    "            gb = ch.functions.sum(gy_nl, axis=0) # gy_nl.sum(axis=0)\n",
    "            out.append(ch.functions.cast(gb, b.dtype))\n",
    "            \n",
    "        # Return just the relevant gradients we appended.\n",
    "        return out\n",
    "\n",
    "\n",
    "def nonlinear(x, W, b):\n",
    "    '''\n",
    "    A nice thin wrapper for our non-linear FunctionNode on\n",
    "    Variable objects.\n",
    "    '''\n",
    "    if b is None:\n",
    "        args = (x, W)\n",
    "    else:\n",
    "        args = (x, W, b)\n",
    "        \n",
    "    y, = NonLinearFunction().apply(args)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinear(ch.Link):\n",
    "    '''\n",
    "    A Link class for our non-linear transformation, implemented\n",
    "    in the NonLinearFunction class.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 in_size, out_size,\n",
    "                 init_W=None, init_b=None,\n",
    "                 init_delta=None,\n",
    "                 nobias=False):\n",
    "        super(NonLinear, self).__init__()\n",
    "        \n",
    "        # Here we initialize and \"register\" the parameters\n",
    "        # of interest. This is critical because when we\n",
    "        # call __call__(x) and apply the underlying affine\n",
    "        # transformations to input x (both forward pass and\n",
    "        # backward pass), the optimization algorithm knows\n",
    "        # that we want to optimize W and maybe b, but not x.\n",
    "\n",
    "        with self.init_scope():\n",
    "            \n",
    "            # If provided an ndarray, use it.\n",
    "            if init_W is not None:\n",
    "                self.W = ch.Parameter(initializer=np.copy(init_W))\n",
    "            \n",
    "            # Else, use a built-in initializer.\n",
    "            else:\n",
    "                W_initializer = ch.initializers.Uniform(scale=init_delta,\n",
    "                                                        dtype=np.float32)\n",
    "                self.W = ch.Parameter(initializer=W_initializer,\n",
    "                                      shape=(out_size, in_size))\n",
    "            \n",
    "            if nobias:\n",
    "                self.b = None\n",
    "            else:\n",
    "                if init_b is not None:\n",
    "                    self.b = ch.Parameter(initializer=np.copy(init_b))\n",
    "                else:\n",
    "                    self.b = ch.Parameter(initializer=0,\n",
    "                                          shape=(out_size,))\n",
    "                \n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        This method actually applies the linear layer to\n",
    "        inputs x.\n",
    "        '''\n",
    "        return nonlinear(x, self.W, self.b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chain_NonLinReg(ch.Chain):\n",
    "    '''\n",
    "    A simple feed-forward neural network that has\n",
    "    one hidden layer, with non-linear activation.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 out_l0,\n",
    "                 out_l1,\n",
    "                 out_l2,\n",
    "                 init_W1=None,\n",
    "                 init_W2=None,\n",
    "                 init_delta=1.0,\n",
    "                 nobias=False):\n",
    "        super(Chain_NonLinReg, self).__init__()\n",
    "        \n",
    "        with self.init_scope():\n",
    "            self.l1 = NonLinear(in_size=out_l0,\n",
    "                                out_size=out_l1,\n",
    "                                init_W=init_W1,\n",
    "                                init_b=None,\n",
    "                                init_delta=init_delta,\n",
    "                                nobias=True)\n",
    "            \n",
    "            self.l2 = Linear(in_size=out_l1,\n",
    "                             out_size=out_l2,\n",
    "                             init_W=init_W2,\n",
    "                             init_b=None,\n",
    "                             init_delta=init_delta,\n",
    "                             nobias=True)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Chain_Logistic(ch.Chain):\n",
    "    '''\n",
    "    For pre-built activation functions, we can just pass\n",
    "    linear layers through them, and Chainer does the rest.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 out_l0,\n",
    "                 out_l1,\n",
    "                 out_l2,\n",
    "                 init_W1=None,\n",
    "                 init_W2=None,\n",
    "                 init_delta=1.0,\n",
    "                 nobias=False):\n",
    "        super(Chain_Logistic, self).__init__()\n",
    "        \n",
    "        with self.init_scope():\n",
    "            \n",
    "            self.l1 = Linear(in_size=out_l0,\n",
    "                             out_size=out_l1,\n",
    "                             init_W=init_W1,\n",
    "                             init_b=None,\n",
    "                             init_delta=init_delta,\n",
    "                             nobias=True)\n",
    "            \n",
    "            self.l2 = Linear(in_size=out_l1,\n",
    "                             out_size=out_l2,\n",
    "                             init_W=init_W2,\n",
    "                             init_b=None,\n",
    "                             init_delta=init_delta,\n",
    "                             nobias=True)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        out = ch.functions.sigmoid(self.l1(x)) # logistic sigmoid.\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs (us vs. them), n = 3 samples:\n",
      "-----\n",
      "[0.87012995 3.47568442]\n",
      "[0.87012995 3.47568442]\n",
      "-----\n",
      "[0.24158042 2.7378033 ]\n",
      "[0.24158042 2.7378033 ]\n",
      "-----\n",
      "[2.52917936 8.49487156]\n",
      "[2.52917936 8.49487156]\n",
      "-----\n",
      "Gradient comparison:\n",
      "--\n",
      "us: [[ 1.2020643 -1.8916731]\n",
      " [ 2.1698744 -2.6294236]\n",
      " [ 3.217658  -3.2660933]\n",
      " [ 4.058814  -3.896445 ]]\n",
      "them: [[ 1.2020643 -1.8916731]\n",
      " [ 2.1698744 -2.6294236]\n",
      " [ 3.217658  -3.2660933]\n",
      " [ 4.058814  -3.896445 ]]\n",
      "--\n",
      "us: [[0.9181247  0.6589377  0.58746296 0.602342  ]\n",
      " [0.9181247  0.6589377  0.58746296 0.602342  ]]\n",
      "them: [[0.9181247  0.6589377  0.58746296 0.602342  ]\n",
      " [0.9181247  0.6589377  0.58746296 0.602342  ]]\n",
      "--\n",
      "Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "d0, d1, d2 = 2, 4, 2\n",
    "\n",
    "init_W1 = np.arange(d0*d1, dtype=np.float32).reshape((d1,d0))\n",
    "init_W2 = np.arange(d1*d2, dtype=np.float32).reshape((d2,d1))\n",
    "\n",
    "mod_us = Chain_NonLinReg(out_l0=d0, out_l1=d1, out_l2=d2,\n",
    "                         init_W1=init_W1, init_W2=init_W2)\n",
    "mod_them = Chain_Logistic(out_l0=d0, out_l1=d1, out_l2=d2,\n",
    "                         init_W1=init_W1, init_W2=init_W2)\n",
    "\n",
    "X = np.random.normal(loc=0.0, scale=1.0, size=(n,d0))\n",
    "\n",
    "y_us = mod_us(X)\n",
    "y_them = mod_them(X)\n",
    "\n",
    "print(\"Outputs (us vs. them), n =\", n, \"samples:\")\n",
    "print(\"-----\")\n",
    "for i in range(n):\n",
    "    print(y_us.data[i,:])\n",
    "    print(y_them.data[i,:])\n",
    "    print(\"-----\")\n",
    "    \n",
    "# Gradient compuations.\n",
    "y_us.grad = np.ones(y_us.data.shape, dtype=y_us.data.dtype)\n",
    "y_them.grad = np.ones(y_them.data.shape, dtype=y_them.data.dtype)\n",
    "mod_us.cleargrads()\n",
    "mod_them.cleargrads()\n",
    "y_us.backward()\n",
    "y_them.backward()\n",
    "\n",
    "print(\"Gradient comparison:\")\n",
    "print(\"--\")\n",
    "diff_sq = 0\n",
    "zipped = zip(mod_us.params(), mod_them.params())\n",
    "for p_us, p_them in zipped:\n",
    "    grad_us = p_us.grad\n",
    "    grad_them = p_them.grad\n",
    "    diff_sq += np.linalg.norm(grad_us-grad_them)**2\n",
    "    print(\"us:\", grad_us)\n",
    "    print(\"them:\", grad_them)\n",
    "    print(\"--\")\n",
    "    \n",
    "print(\"Difference:\", math.sqrt(diff_sq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要点：\n",
    "\n",
    " - 活性化関数がChainerの`functions`モジュールに標準搭載されている場合は、わざわざ自分の`NonLinearFunction`を作る必要はなく、線形`Link`をそのまま非線形変換に渡すだけで済むのである。\n",
    " \n",
    " - 一方、Chainerにあらかじめ用意されていない活性化関数を使いたいときは、上記の段取りを模倣することで、任意の微分可能な非線形変換に対応した`Link`を整備することが容易にできる。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chainer_lgstreg\"></a>\n",
    "### 多クラスのロジスティック回帰の自作と比較\n",
    "\n",
    "<a href=\"Classifiers.ipynb\">前の章</a>では、自作のロジスティック回帰を用意した。ここではChainerを使ったロジスティック回帰の学習機を作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tables\n",
    "import numpy as np\n",
    "import chainer as ch\n",
    "import multiprocessing as mltp\n",
    "\n",
    "import dataclass\n",
    "import algorithms\n",
    "import helpers as hlp\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental setup.\n",
    "\n",
    "task_name = \"iris\" # SET BY HAND.\n",
    "init_delta = 5.0 # controls support of random initialization\n",
    "num_trials = 250 # number of independent random trials to conduct\n",
    "\n",
    "# Prepare a results folder, if doesn't already exist.\n",
    "hlp.makedir_safe(os.path.join(\"results\", task_name))\n",
    "\n",
    "# Establish file connection.\n",
    "toread = os.path.join(\"data\", task_name, \"data.h5\")\n",
    "f = tables.open_file(toread, mode=\"r\")\n",
    "\n",
    "# Data for the hand-built model.\n",
    "data = dataclass.DataSet()\n",
    "data.init_tr(X=f.root.train.inputs.read(),\n",
    "             y=f.root.train.labels.read())\n",
    "data.init_te(X=f.root.test.inputs.read(),\n",
    "             y=f.root.test.labels.read())\n",
    "\n",
    "# Data for Chainer model.\n",
    "Z_tr = ch.datasets.TupleDataset(np.float32(f.root.train.inputs.read()),\n",
    "                                np.int8(f.root.train.labels.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment setup.\n",
    "\n",
    "n, numfeats = data.X_tr.shape # sample size, number of features.\n",
    "nc = 3 # number of classes\n",
    "d = (nc-1)*numfeats\n",
    "\n",
    "# Algorithm-related.\n",
    "m_idx_todo = [0,1] # let's us manually pick and choose which methods to evaluate.\n",
    "t_max = 100 # termination condition; maximum number of iterations.\n",
    "thres = -1.0 # termination condition; if negative, runs for max iterations.\n",
    "\n",
    "def alpha_fixed(t, val): # step-size callback function.\n",
    "    return val\n",
    "def make_step(u):\n",
    "    def mystep(t, model=None, data=None, newdir=None):\n",
    "        return alpha_fixed(t=t, val=u)\n",
    "    return mystep\n",
    "alphaval = 0.1\n",
    "\n",
    "# Clerical.\n",
    "mth_names = [\"lgstReg\", \"lgstReg-ch\"]\n",
    "num_mths = len(mth_names)\n",
    "mth_colours = [\"black\", \"blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the algorithms.\n",
    "\n",
    "# Prepare storage for performance metrics.\n",
    "riskvals = np.zeros((num_trials,t_max,num_mths), dtype=np.float32)\n",
    "loss_tr = np.zeros((num_trials,t_max,num_mths), dtype=np.float32)\n",
    "\n",
    "# Loop over trials.\n",
    "for tri in range(num_trials):\n",
    "    \n",
    "    # Initial weight settings.\n",
    "    w_init = np.random.uniform(low=-init_delta, high=init_delta, size=d).reshape((d,1))\n",
    "    w_init = np.float32(w_init)\n",
    "    W_init_ch = np.concatenate((w_init.flatten().reshape((nc-1,numfeats)),\n",
    "                                np.zeros((1,numfeats), dtype=np.float32)),\n",
    "                               axis=0) # add an extra row of zeros to start.\n",
    "    \n",
    "    # Initialize models (hand-built).\n",
    "    mod_learner = models.LogisticReg(data=data)\n",
    "    risk_star = 0 # lower bound on optimal risk value.\n",
    "    loss_star = 0\n",
    "    \n",
    "    # Initialize models (Chainer-based).\n",
    "    mod_chainer = Chain_LinReg(out_l0=numfeats,\n",
    "                               out_l1=nc,\n",
    "                               init_W=W_init_ch,\n",
    "                               init_b=None,\n",
    "                               init_delta=init_delta,\n",
    "                               nobias=True)\n",
    "    \n",
    "    # Initialize algorithms (hand-built).\n",
    "    al_gd = algorithms.Algo_GD(w_init=w_init,\n",
    "                               step=make_step(alphaval),\n",
    "                               t_max=t_max,\n",
    "                               thres=thres,\n",
    "                               store=True,\n",
    "                               lamreg=None)\n",
    "    \n",
    "    # Initialize algorithms (Chainer-based).\n",
    "    opt_chainer = ch.optimizers.SGD(lr=alphaval)\n",
    "    opt_chainer.setup(mod_chainer) # pass model!\n",
    "\n",
    "    \n",
    "    # Run all algorithms and save their performance.\n",
    "    \n",
    "    ## ERM-GD.\n",
    "    mthidx = 0\n",
    "    if mthidx in m_idx_todo:        \n",
    "        idx = 0\n",
    "        for mystep in al_gd:\n",
    "            al_gd.update(model=mod_learner, data=data)\n",
    "            # Record performance\n",
    "            loss_tr[tri,idx,mthidx] = np.mean(mod_learner.l_tr(w=al_gd.w, data=data))-loss_star\n",
    "            y_est_te = mod_learner.classify(w=al_gd.w, X=data.X_te)\n",
    "            riskvals[tri,idx,mthidx] = mod_learner.class_perf(y_est=y_est_te,\n",
    "                                                              y_true=data.y_te)[\"rate\"]-risk_star\n",
    "            idx += 1\n",
    "        \n",
    "    ## Replication of ERM using Chainer.\n",
    "    mthidx = 1\n",
    "    if mthidx in m_idx_todo:\n",
    "        idx = 0\n",
    "        iter_train = ch.iterators.SerialIterator(dataset=Z_tr,\n",
    "                                                 batch_size=n, # thus SGD=GD; deterministic.\n",
    "                                                 repeat=True,\n",
    "                                                 shuffle=False)\n",
    "        while iter_train.epoch < t_max:\n",
    "\n",
    "            # Get our mini-batch.\n",
    "            Z_batch = iter_train.next()\n",
    "            X_batch, y_batch = ch.dataset.concat_examples(Z_batch)\n",
    "\n",
    "            # Get the un-normalized log-probability outputs.\n",
    "            prediction_tr = mod_chainer(X_batch)\n",
    "            \n",
    "            #np.concatenate((mod_chainer(X_batch).data,\n",
    "            #                                np.zeros((X_batch.shape[0],1), dtype=np.float32)),\n",
    "            #                               axis=1) # need to append zeros for 1-probs.\n",
    "\n",
    "            # Loss computations (will feed the grad computations).\n",
    "            loss = ch.functions.softmax_cross_entropy(x=prediction_tr,\n",
    "                                                      t=np.int8(data.y_tr).flatten(),\n",
    "                                                      normalize=True,\n",
    "                                                      reduce=\"mean\")\n",
    "            \n",
    "            # Gradient computations.\n",
    "            mod_chainer.cleargrads()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Parameter updates.\n",
    "            opt_chainer.update()\n",
    "            \n",
    "            # Record performance.\n",
    "            loss_tr[tri,idx,mthidx] = loss.data-loss_star\n",
    "            prediction_te = mod_chainer(data.X_te)\n",
    "            accuracy = ch.functions.accuracy(y=prediction_te,\n",
    "                                             t=np.int8(data.y_te).flatten()).data\n",
    "            \n",
    "            riskvals[tri,idx,mthidx] = 1.0-accuracy-risk_star\n",
    "            idx += 1\n",
    "\n",
    "\n",
    "# Finally, take statistics of the performance metrics over all trials.\n",
    "ave_loss_tr = np.mean(loss_tr, axis=0)\n",
    "ave_riskvals = np.mean(riskvals, axis=0)\n",
    "sd_loss_tr = np.std(loss_tr, axis=0)\n",
    "sd_riskvals = np.std(riskvals, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一つ注目すべきは、`Chain_LinReg`をそのまま使っていることである。前の節の文脈では、このモデルを線形回帰のために整備したのだが、ロジスティック回帰に必要な演算はすべてカバーされているので、問題なく使える。回帰モデルとの違いがどこにあるかというと、今回は実数スカラーではなく、多次元ベクトルを出していることと、その出力がsoftmax cross-entropyに渡されていることである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualization of performance.\n",
    "\n",
    "tvals = np.arange(t_max)+1 # better to start from the first update.\n",
    "\n",
    "# Average over trials.\n",
    "myfig = plt.figure(figsize=(15,5))\n",
    "\n",
    "ax_loss_tr = myfig.add_subplot(1, 3, 1)\n",
    "for m in m_idx_todo:\n",
    "    vals = ave_loss_tr[:,m]\n",
    "    err = sd_loss_tr[:,m]\n",
    "    ax_loss_tr.plot(tvals, vals, color=mth_colours[m], label=mth_names[m])\n",
    "    ax_loss_tr.legend(loc=1,ncol=1)\n",
    "    plt.axhline(y=0.0, color=\"black\")\n",
    "    plt.title(\"Primal loss (training)\")\n",
    "\n",
    "ax_riskvals = myfig.add_subplot(1, 3, 2)\n",
    "for m in m_idx_todo:\n",
    "    vals = ave_riskvals[:,m]\n",
    "    err = sd_riskvals[:,m]\n",
    "    err_log = err / vals\n",
    "    ax_riskvals.plot(tvals, vals, \"-\", color=mth_colours[m], label=mth_names[m])\n",
    "    ax_riskvals.legend(loc=1,ncol=1)\n",
    "    plt.axhline(y=0.0, color=\"black\")\n",
    "    plt.title(\"Misclassification rate (test)\")\n",
    "\n",
    "ax_variation = myfig.add_subplot(1, 3, 3)\n",
    "for m in m_idx_todo:\n",
    "    vals = sd_riskvals[:,m]\n",
    "    ax_variation.plot(tvals, vals, \"-\", color=mth_colours[m], label=mth_names[m])\n",
    "    ax_variation.legend(loc=1,ncol=1)\n",
    "    plt.axhline(y=0.0, color=\"black\")\n",
    "    plt.title(\"Variation of error across trials\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the file connection.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__練習問題__\n",
    "\n",
    "0. 基本的な振る舞いが似ているとはいえ、`lgstReg`と`lgstReg-ch`がまったく同じ働きをしているわけではない。これはなぜだろうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chainer_newopt\"></a>\n",
    "### 最適化を自分の手で\n",
    "\n",
    "これまでのChainerを用いた事例では、標準整備された`Optimizer`サブクラスを使っている。具体的には、`SGD`というサブクラスで、確率的勾配降下法を実装したものであるが、これまでの例ではミニバッチの大きさを$n$としているので、結局は決定論的な勾配降下法(GD)であった。\n",
    "\n",
    "最適化法を自分で調整・改造したいときには、主として糸口が2つある。（１）`Optimizer`サブクラスを自分で作って、上と同様に使うこと。（２）そもそも`Optimizer`の枠組みを使わずに、モデルが管理する`Parameter`オブジェクト自体を操作すること。\n",
    "\n",
    "前者ではCPU/GPUハードウェアを駆使することがきわめて楽であるが、自由度が比較的低く、実装できるアルゴリズムが限られてしまう。後者では、考えられる範囲でほとんど何でもできる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自力でGDを行う簡単な例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment setup.\n",
    "\n",
    "# Data-related.\n",
    "data = dataclass.DataSet() # Initialize one data object; will be re-populated at each trial.\n",
    "n = 500 # sample size\n",
    "d = 2 # number of parameters\n",
    "init_delta = 5.0 # controls support of random initialization\n",
    "num_trials = 250 # number of independent random trials to conduct\n",
    "cov_X = np.eye(d) # covariance matrix of the inputs.\n",
    "\n",
    "w_star = np.ones(d).reshape((d,1)) # vector specifying true model\n",
    "\n",
    "# Algorithm-related.\n",
    "m_idx_todo = [0,1] # let's us manually pick and choose which methods to evaluate.\n",
    "t_max = 30 # termination condition; maximum number of iterations.\n",
    "thres = -1.0 # termination condition; if negative, runs for max iterations.\n",
    "\n",
    "def alpha_fixed(t, val): # step-size callback function.\n",
    "    return val\n",
    "def make_step(u):\n",
    "    def mystep(t, model=None, data=None, newdir=None):\n",
    "        return alpha_fixed(t=t, val=u)\n",
    "    return mystep\n",
    "alphaval = 0.25\n",
    "\n",
    "# Clerical.\n",
    "mth_names = [\"gd\", \"gd-ch\"]\n",
    "num_mths = len(mth_names)\n",
    "mth_colours = [\"black\", \"blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make choice of additive noise distribution (un-comment your choice).\n",
    "#paras = {\"name\": \"norm\", \"shift\": 0.0, \"scale\": 20.0}\n",
    "paras = {\"name\": \"lnorm\", \"meanlog\": 0.0, \"sdlog\": 1.75}\n",
    "\n",
    "# Put together risk function.\n",
    "def risk(w):\n",
    "    mean_noise, var_noise = hlp.noise_risk(paras=paras)\n",
    "    return hlp.riskMaker(w=w, A=cov_X, b=math.sqrt(var_noise), w_star=w_star)\n",
    "risk_star = risk(w=w_star) # optimal risk value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the algorithms.\n",
    "\n",
    "# Prepare storage for performance metrics.\n",
    "riskvals = np.zeros((num_trials,t_max,num_mths), dtype=np.float32)\n",
    "loss_tr = np.zeros((num_trials,t_max,num_mths), dtype=np.float32)\n",
    "truedist = np.zeros((num_trials,t_max,num_mths), dtype=np.float32)\n",
    "\n",
    "# Loop over trials.\n",
    "for tri in range(num_trials):\n",
    "    \n",
    "    # Generate new data (with *centered* noise).\n",
    "    X = np.random.normal(loc=0.0, scale=1.0, size=(n,d))\n",
    "    noise = hlp.noise_data(n=n, paras=paras)\n",
    "    y = np.dot(X, w_star) + noise\n",
    "    data.init_tr(X=X, y=y)\n",
    "    \n",
    "    # Data for Chainer model.\n",
    "    Z = ch.datasets.TupleDataset(np.float32(X),\n",
    "                                 np.float32(y))\n",
    "    \n",
    "    # Initial weight settings.\n",
    "    w_init = w_star + np.random.uniform(low=-init_delta, high=init_delta, size=d).reshape((d,1))\n",
    "    w_init = np.float32(w_init)\n",
    "    \n",
    "    # Initialize models (hand-built).\n",
    "    mod_learner = models.LinearL2(data=data)\n",
    "    risk_star = risk(w=w_star) # optimal risk value.\n",
    "    loss_star = np.mean(mod_learner.l_tr(w=w_star, data=data))\n",
    "    \n",
    "    # Initialize models (Chainer-based).\n",
    "    mod_chainer = Chain_LinReg(out_l0=d,\n",
    "                          out_l1=1,\n",
    "                          init_W=w_init.T,\n",
    "                          init_b=None,\n",
    "                          init_delta=init_delta,\n",
    "                          nobias=True)\n",
    "    \n",
    "    # Initialize algorithms (hand-built).\n",
    "    al_gd = algorithms.Algo_GD(w_init=w_init,\n",
    "                               step=make_step(alphaval),\n",
    "                               t_max=t_max,\n",
    "                               thres=thres,\n",
    "                               store=True,\n",
    "                               lamreg=None)\n",
    "\n",
    "    \n",
    "    # Run all algorithms and save their performance.\n",
    "    \n",
    "    ## ERM-GD.\n",
    "    mthidx = 0\n",
    "    if mthidx in m_idx_todo:        \n",
    "        idx = 0\n",
    "        for mystep in al_gd:\n",
    "            al_gd.update(model=mod_learner, data=data)\n",
    "            # Record performance\n",
    "            loss_tr[tri,idx,mthidx] = np.mean(mod_learner.l_tr(w=al_gd.w, data=data))-loss_star\n",
    "            riskvals[tri,idx,mthidx] = risk(w=al_gd.w)-risk_star\n",
    "            truedist[tri,idx,mthidx] = np.linalg.norm(w_star-al_gd.w)-0\n",
    "            idx += 1\n",
    "        \n",
    "        \n",
    "    ## Replication of ERM using Chainer.\n",
    "    mthidx = 1\n",
    "    if mthidx in m_idx_todo:\n",
    "        idx = 0\n",
    "        iter_train = ch.iterators.SerialIterator(dataset=Z,\n",
    "                                                 batch_size=n, # thus SGD=GD; deterministic.\n",
    "                                                 repeat=True,\n",
    "                                                 shuffle=False)\n",
    "        while iter_train.epoch < t_max:\n",
    "\n",
    "            # Get our mini-batch.\n",
    "            Z_batch = iter_train.next()\n",
    "            X_batch, y_batch = ch.dataset.concat_examples(Z_batch)\n",
    "\n",
    "            # Predictions.\n",
    "            prediction_tr = mod_chainer(X_batch)\n",
    "\n",
    "            # Loss computations (will feed the grad computations).\n",
    "            loss = ch.functions.mean_squared_error(prediction_tr, y_batch) / 2.0\n",
    "            loss_star = np.mean(mod_learner.l_tr(w=w_star, data=data))\n",
    "            \n",
    "            # Gradient computations.\n",
    "            mod_chainer.cleargrads()\n",
    "            loss.backward()\n",
    "\n",
    "            # Parameter updates.\n",
    "            for p in mod_chainer.params():\n",
    "                grad = p.grad\n",
    "                if grad is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    p.data -= alphaval * grad\n",
    "            \n",
    "            # Record performance\n",
    "            loss_tr[tri,idx,mthidx] = np.mean(mod_learner.l_tr(w=mod_chainer.l1.W.data.T, data=data))-loss_star\n",
    "            riskvals[tri,idx,mthidx] = risk(w=mod_chainer.l1.W.data.T)-risk_star\n",
    "            truedist[tri,idx,mthidx] = np.linalg.norm(w_star-mod_chainer.l1.W.data.T)-0\n",
    "            idx += 1\n",
    "\n",
    "\n",
    "# Finally, take statistics of the performance metrics over all trials.\n",
    "ave_loss_tr = np.mean(loss_tr, axis=0)\n",
    "ave_riskvals = np.mean(riskvals, axis=0)\n",
    "ave_truedist = np.mean(truedist, axis=0)\n",
    "sd_loss_tr = np.std(loss_tr, axis=0)\n",
    "sd_riskvals = np.std(riskvals, axis=0)\n",
    "sd_truedist = np.std(truedist, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualization of performance.\n",
    "\n",
    "tvals = np.arange(t_max)+1 # better to start from the first update.\n",
    "\n",
    "# Average over trials.\n",
    "myfig = plt.figure(figsize=(15,5))\n",
    "\n",
    "ax_loss_tr = myfig.add_subplot(1, 3, 1)\n",
    "for m in m_idx_todo:\n",
    "    vals = ave_loss_tr[:,m]\n",
    "    err = sd_loss_tr[:,m]\n",
    "    ax_loss_tr.plot(tvals, vals, color=mth_colours[m], label=mth_names[m])\n",
    "    #ax_loss_tr.errorbar(tvals, vals, yerr=err, fmt='-o', col=mth_colours[m], label=mth_names[m])\n",
    "    ax_loss_tr.legend(loc=1,ncol=1)\n",
    "    plt.axhline(y=0.0, color=\"black\")\n",
    "    plt.title(\"Excess empirical risk\")\n",
    "    \n",
    "\n",
    "ax_riskvals = myfig.add_subplot(1, 3, 2)\n",
    "for m in m_idx_todo:\n",
    "    vals = ave_riskvals[:,m]\n",
    "    err = sd_riskvals[:,m]\n",
    "    err_log = err / vals\n",
    "    ax_riskvals.semilogy(tvals, vals, \"-o\", color=mth_colours[m], label=mth_names[m])\n",
    "    ax_riskvals.legend(loc=1,ncol=1)\n",
    "    plt.axhline(y=0.0, color=\"black\")\n",
    "    plt.title(\"Excess risk\")\n",
    "\n",
    "ax_variation = myfig.add_subplot(1, 3, 3)\n",
    "for m in m_idx_todo:\n",
    "    vals = sd_riskvals[:,m]\n",
    "    ax_variation.plot(tvals, vals, \"-o\", color=mth_colours[m], label=mth_names[m])\n",
    "    ax_variation.legend(loc=1,ncol=1)\n",
    "    plt.axhline(y=0.0, color=\"black\")\n",
    "    plt.title(\"Variation of risk across samples\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__練習問題__\n",
    "\n",
    "0. 上のコードでは決定論的な勾配降下法を`Optimizer`枠組みの外で実装した。これを踏まえて、任意のミニバッチに対応できる確率的勾配降下法（SGD)に拡張すること。また、これが事前に指定する$T$回だけ反復するようにすること（注：`iter_train.epoch`の箇所は修正が必要）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
