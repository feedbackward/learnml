{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of raw data for encoding task\n",
    "\n",
    "__Contents:__\n",
    "- <a href=\"#overview\">Overview of the data set</a>\n",
    "- <a href=\"#stim_check\">Examining the stimulus data</a>\n",
    "- <a href=\"#stim_ds\">Spatially downsampling the stimulus</a>\n",
    "- <a href=\"#resp_check\">Examining the BOLD response data</a>\n",
    "- <a href=\"#cleanidx_tr\">Regarding pre-processing of responses</a>\n",
    "\n",
    "___\n",
    "\n",
    "We shall have two categorically distinct kinds of data: visual stimulus in the form of natural movies, and blood oxygen level-dependent (BOLD) signals measured using functional magnetic resonance imaging (fMRI), as an indicator of brain activity.\n",
    "\n",
    "<img src=\"img/stimuli_ex.png\" alt=\"Stimuli Image\" width=\"240\" height=\"180\" />\n",
    "\n",
    "<img src=\"img/fMRI_example.png\" alt=\"fMRI Image\" width=\"240\" height=\"180\" />\n",
    "\n",
    "Our chief goal will be to create a working *encoder*, which predicts brain activity based on the contents of visual stimulus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Overview of the data set\n",
    "\n",
    "The data set of interest, called \"Gallant Lab Natural Movie 4T fMRI Data set\" (*vim-2*) contains the following types of data (of interest to us today):\n",
    "\n",
    "__Stimuli__\n",
    "```\n",
    "Stimuli.tar.gz 4089110463 (3.8 GB)\n",
    "```\n",
    "\n",
    "__BOLD response__\n",
    "```\n",
    "VoxelResponses_subject1.tar.gz 3178624411 (2.9 GB)\n",
    "VoxelResponses_subject2.tar.gz 3121761551 (2.9 GB)\n",
    "VoxelResponses_subject3.tar.gz 3216874972 (2.9 GB)\n",
    "```\n",
    "\n",
    "All stimuli are stored in the `Stimuli.mat` file, Matlab v.7.3 format. Variables take the following forms.\n",
    "\n",
    " - `st`: training stimuli. 128x128x3x108000 matrix (108000 128x128 rgb frames). \n",
    " - `sv`: validation stimuli. 128x128x3x8100 matrix (8100 128x128 rgb frames).\n",
    "\n",
    "For the training data, stimulus was presented at 15fps over 7200 timepoints = 120 minutes, for a total of 108000 frames. For the validation data, stimulus was presented at 15fps over 540 timepoints = 9 minutes for a total of 8100 frames.\n",
    "\n",
    "The validation stimulus was presented a total of ten times to each subject, and the response values used above correspond to the average taken over these trials. The \"raw\" validation response values (pre-averaging) are available, but the mean values serve our purposes just fine.\n",
    "\n",
    "Finally, no re-arranging of the data is required, fortunately. The authors note:\n",
    "\n",
    "> *\"The order of the stimuli in the \"st\" and \"sv\" variables matches the order of the stimuli in the \"rt\" and \"rv\" variables in the response files.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, the first task is to decompress the data.\n",
    "\n",
    "```\n",
    "$ tar -xzf Stimuli.tar.gz\n",
    "$ tar -xzf VoxelResponses_subject1.tar.gz\n",
    "$ tar -xzf VoxelResponses_subject2.tar.gz\n",
    "$ tar -xzf VoxelResponses_subject3.tar.gz\n",
    "```\n",
    "\n",
    "This leaves us with files `Stimuli.mat` and `VoxelResponses_subject{1,2,3}.mat`. Our key tool of use here will be the __PyTables__ library (http://www.pytables.org/usersguide/index.html), which is convenient for hierarchical data. To see what is stored in the data (and corroborate with the documentation), run the following.\n",
    "\n",
    "```\n",
    "$ ptdump Stimuli.mat\n",
    "/ (RootGroup) ''\n",
    "/st (EArray(108000, 3, 128, 128), zlib(3)) ''\n",
    "/sv (EArray(8100, 3, 128, 128), zlib(3)) ''\n",
    "```\n",
    "Note the coordinates. There is not much of a hierarchy here, just two folders in the root group. Note the order of the dimensions; dim 1 is the observation index, dim 2 is the RGB channel, and dims 3 and 4 specify pixel position in an array.\n",
    "\n",
    "The response data has a richer structure.\n",
    "\n",
    "```\n",
    "$ ptdump VoxelResponses_subject1.mat \n",
    "/ (RootGroup) ''\n",
    "/rt (EArray(73728, 7200), zlib(3)) ''\n",
    "/rv (EArray(73728, 540), zlib(3)) ''\n",
    "/rva (EArray(73728, 10, 540), zlib(3)) ''\n",
    "(...Warnings...)\n",
    "/ei (Group) ''\n",
    "/ei/TRsec (Array(1, 1)) ''\n",
    "/ei/datasize (Array(3, 1)) ''\n",
    "/ei/imhz (Array(1, 1)) ''\n",
    "/ei/valrepnum (Array(1, 1)) ''\n",
    "/roi (Group) ''\n",
    "/roi/FFAlh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/FFArh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/IPlh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/IPrh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/MTlh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/MTplh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/MTprh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/MTrh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/OBJlh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/OBJrh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/PPAlh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/PPArh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/RSCrh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/STSrh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/VOlh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/VOrh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/latocclh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/latoccrh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v1lh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v1rh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v2lh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v2rh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v3alh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v3arh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v3blh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v3brh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v3lh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v3rh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v4lh (EArray(18, 64, 64), zlib(3)) ''\n",
    "/roi/v4rh (EArray(18, 64, 64), zlib(3)) ''\n",
    "\n",
    "```\n",
    "\n",
    "Within the root group, we have arrays `rt`, `rv`, and `rva` which contain BOLD response data, group `roi` which contains arrays used as indices for assigning particular responses to particular voxels, and group `ei` which stores pertinent experimental information. In particular, under the `roi` group there are numerous arrays named according to anatomical/functional regions of interest (ROI). For example, `v4rh` corresponds to the __V4__ region in the __l__eft __h__emisphere. Since there are far less ROIs than there are voxels $18 \\times 64 \\times 64 = 73728$, note that each region is composed of many voxels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"stim_check\"></a>\n",
    "## Examining the stimulus data\n",
    "\n",
    "Let's begin by taking a look inside the stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tables\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pprint as pp\n",
    "\n",
    "# Open file connection.\n",
    "f = tables.open_file(\"data/vim-2/Stimuli.mat\", mode=\"r\")\n",
    "\n",
    "# Get object and array.\n",
    "stimulus_object = f.get_node(where=\"/\", name=\"sv\")\n",
    "print(\"stimulus_object:\")\n",
    "print(stimulus_object)\n",
    "print(type(stimulus_object))\n",
    "print(\"----\")\n",
    "\n",
    "stimulus_array = stimulus_object.read()\n",
    "print(\"stimulus_array:\")\n",
    "#print(stimulus_array)\n",
    "print(type(stimulus_array))\n",
    "print(\"----\")\n",
    "\n",
    "# Close the connection.\n",
    "f.close()\n",
    "\n",
    "# Check that it is closed.\n",
    "if not f.isopen:\n",
    "    print(\"Successfully closed.\")\n",
    "else:\n",
    "    print(\"File connection is still open.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the content of this array to ensure it has the content that we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file connection.\n",
    "f = tables.open_file(\"data/vim-2/Stimuli.mat\", mode=\"r\")\n",
    "\n",
    "# Get object and array.\n",
    "stimulus_object = f.get_node(where=\"/\", name=\"sv\")\n",
    "print(\"stimulus_object:\")\n",
    "print(stimulus_object)\n",
    "print(type(stimulus_object))\n",
    "print(\"----\")\n",
    "\n",
    "stimulus_array = stimulus_object.read()\n",
    "print(\"stimulus_array:\")\n",
    "#print(stimulus_array)\n",
    "print(\"(type)\")\n",
    "print(type(stimulus_array))\n",
    "print(\"(dtype)\")\n",
    "print(stimulus_array.dtype)\n",
    "print(\"----\")\n",
    "\n",
    "num_frames = stimulus_array.shape[0]\n",
    "num_channels = stimulus_array.shape[1]\n",
    "frame_w = stimulus_array.shape[2]\n",
    "frame_h = stimulus_array.shape[3]\n",
    "\n",
    "frames_to_play = 5\n",
    "\n",
    "oneframe = np.zeros(num_channels*frame_h*frame_w, dtype=np.uint8).reshape((frame_h, frame_w, num_channels))\n",
    "im = plt.imshow(oneframe)\n",
    "\n",
    "for t in range(frames_to_play):\n",
    "    oneframe[:,:,0] = stimulus_array[t,0,:,:] # red\n",
    "    oneframe[:,:,1] = stimulus_array[t,1,:,:] # green\n",
    "    oneframe[:,:,2] = stimulus_array[t,2,:,:] # blue\n",
    "    plt.imshow(oneframe)\n",
    "    plt.show()\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually view this as a video, execute the following script from the command line (assuming the current directory is `learnml`).\n",
    "\n",
    "```\n",
    "python ./support/vidcheck1.py\n",
    "```\n",
    "\n",
    "Anyways, clearly the content is roughly as we would expect (a series of video clips), although the orientation is incorrect, and the framerate is rather high.\n",
    "\n",
    "\n",
    "### Exercises (A):\n",
    "\n",
    "0. Swap the coordinates (see help(np.swapaxes)) to fix the orientation.\n",
    "\n",
    "0. In addition to the first handful of frames, check the *last* handful of frames to ensure that the content is as we expect.\n",
    "\n",
    "0. Repeat the visualization procedure above for `st` (the training data) to ensure it is also as we expect.\n",
    "\n",
    "0. Do the above procedures for the first 100 frames. Note that the current framerate is 15fps. By modifying the `for` loop above, using `np.arange` instead of `range`, we can easily \"down-sample\" to a lower temporal frequency. Try downsampling to a rate of 1fps (by displaying one out of every fifteen frames).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"stim_ds\"></a>\n",
    "## Spatially downsampling the stimulus\n",
    "\n",
    "For computational purposes, it is advisable to spatially down-sample the stimulus data, in addition to the temporal down-sampling required to align it with the BOLD signal responses (this can be done later). Here we carry out the spatial down-sampling. Using the function `resize` from the `transform` module of __scikit-image__, this can be done easily. First, a simple test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage import transform as trans\n",
    "import imageio\n",
    "\n",
    "im = imageio.imread(\"img/bishop.png\") # a 128px x 128px image\n",
    "\n",
    "med_h = 96 # desired height in pixels\n",
    "med_w = 96 # desired width in pixels\n",
    "im_med = trans.resize(image=im, output_shape=(med_h,med_w), mode=\"reflect\")\n",
    "\n",
    "small_h = 32 # desired height in pixels\n",
    "small_w = 32 # desired width in pixels\n",
    "im_small = trans.resize(image=im, output_shape=(small_h,small_w), mode=\"reflect\")\n",
    "\n",
    "tiny_h = 16 # desired height in pixels\n",
    "tiny_w = 16 # desired width in pixels\n",
    "im_tiny = trans.resize(image=im, output_shape=(tiny_h,tiny_w), mode=\"reflect\")\n",
    "\n",
    "myfig = plt.figure(figsize=(18,4))\n",
    "\n",
    "ax_im = myfig.add_subplot(1,4,1)\n",
    "plt.imshow(im)\n",
    "plt.title(\"Original image\")\n",
    "ax_med = myfig.add_subplot(1,4,2)\n",
    "plt.imshow(im_med)\n",
    "plt.title(\"Resized image (Medium)\")\n",
    "ax_small = myfig.add_subplot(1,4,3)\n",
    "plt.imshow(im_small)\n",
    "plt.title(\"Resized image (Small)\")\n",
    "ax_small = myfig.add_subplot(1,4,4)\n",
    "plt.imshow(im_tiny)\n",
    "plt.title(\"Resized image (Tiny)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything appears to be working as expected. Let us now do this for the entire set of visual stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Open file connection.\n",
    "f = tables.open_file(\"data/vim-2/Stimuli.mat\", mode=\"r\")\n",
    "\n",
    "# Get object and array.\n",
    "stimulus_object = f.get_node(where=\"/\", name=\"sv\")\n",
    "stimulus_array = stimulus_object.read()\n",
    "num_frames = stimulus_array.shape[0]\n",
    "num_channels = stimulus_array.shape[1]\n",
    "\n",
    "# Swap the axes.\n",
    "print(\"stimulus array (before):\", stimulus_array.shape)\n",
    "stimulus_array = np.swapaxes(a=stimulus_array, axis1=0, axis2=3)\n",
    "stimulus_array = np.swapaxes(a=stimulus_array, axis1=1, axis2=2)\n",
    "print(\"stimulus array (after):\", stimulus_array.shape)\n",
    "\n",
    "# Downsampled height and width.\n",
    "ds_h = 96\n",
    "ds_w = 96\n",
    "\n",
    "stimulus_array_ds = np.zeros(num_frames*num_channels*ds_h*ds_w,\\\n",
    "                             dtype=np.float32).reshape((ds_h, ds_w, num_channels, num_frames))\n",
    "\n",
    "for t in range(num_frames):\n",
    "    stimulus_array_ds[:,:,:,t] = trans.resize(image=stimulus_array[:,:,:,t],\n",
    "                                              output_shape=(ds_h,ds_w),\n",
    "                                              mode=\"reflect\")\n",
    "    if t % 500 == 0:\n",
    "        print(\"Update: t =\", t)\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that resizing the images yields pixel values on $[0,1]$ rather than $\\{0,1,\\ldots,255\\}$, which is why we set the `dtype` to `np.float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Pre-downsize) max:\", np.max(stimulus_array),\n",
    "      \"min:\", np.min(stimulus_array),\n",
    "      \"ave:\", np.mean(stimulus_array))\n",
    "print(\"(Post-downsize) max:\", np.max(stimulus_array_ds),\n",
    "      \"min:\", np.min(stimulus_array_ds),\n",
    "      \"ave:\", np.mean(stimulus_array_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now save this as a binary file for fast reading/writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"data/vim-2/X_te.dat\"\n",
    "dtype = stimulus_array_ds.dtype\n",
    "shape = stimulus_array_ds.shape\n",
    "with open(fname, mode=\"bw\") as fbin:\n",
    "    stimulus_array_ds.tofile(fbin)\n",
    "    print(\"Saved to file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To ensure that everything worked as desired, load it up and look at the first few frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Wipe the downsampled array.\n",
    "stimulus_array_ds = np.zeros(num_frames*num_channels*ds_h*ds_w,\\\n",
    "                             dtype=np.float32).reshape((ds_h, ds_w, num_channels, num_frames))\n",
    "\n",
    "# Load up the stored array.\n",
    "with open(fname, mode=\"br\") as fbin:\n",
    "    print(\"Reading...\", end=\" \")\n",
    "    stimulus_array_ds = np.fromfile(file=fbin, dtype=dtype).reshape(shape)\n",
    "    print(\"OK.\")\n",
    "\n",
    "# Check a few frames.\n",
    "num_frames = stimulus_array_ds.shape[3]\n",
    "\n",
    "frames_to_play = 5\n",
    "\n",
    "for t in range(frames_to_play):\n",
    "    plt.imshow(stimulus_array_ds[:,:,:,t])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercises (B):\n",
    "\n",
    "0. Repeat the above down-sampling and writing to disk for `st`, the training data, saving this as `X_tr` (as a companion to `X_te` saved above).\n",
    "\n",
    "0. For potential investigation later on, prepare a *very* small version of the stimulus, say $32 \\times 32$ pixels, saved using the names `X_tr_32px` and `X_tr_32px`. What do you think the potential benefits are of doing this? What about potential drawbacks?\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"resp_check\"></a>\n",
    "## Examining the BOLD response data\n",
    "\n",
    "Next, we examine the response data (feel free to reset the kernel here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tables\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pprint as pp\n",
    "\n",
    "# Open file connection.\n",
    "f = tables.open_file(\"data/vim-2/VoxelResponses_subject1.mat\", mode=\"r\")\n",
    "\n",
    "# Get objects and arrays.\n",
    "\n",
    "response_object = f.get_node(where=\"/\", name=\"rv\")\n",
    "idx_object = f.get_node(where=\"/roi/\", name=\"v4lh\")\n",
    "\n",
    "print(\"response_object:\")\n",
    "print(response_object)\n",
    "print(type(response_object))\n",
    "print(\"----\")\n",
    "print(\"idx_object:\")\n",
    "print(idx_object)\n",
    "print(type(idx_object))\n",
    "print(\"----\")\n",
    "\n",
    "response_array = response_object.read()\n",
    "idx_array = idx_object.read()\n",
    "\n",
    "print(\"response_array:\")\n",
    "#print(response_array)\n",
    "print(type(response_array))\n",
    "print(\"----\")\n",
    "print(\"idx_array:\")\n",
    "#print(idx_array)\n",
    "print(type(idx_array))\n",
    "print(\"----\")\n",
    "\n",
    "# Close the connection.\n",
    "f.close()\n",
    "\n",
    "# Check that it is closed.\n",
    "if not f.isopen:\n",
    "    print(\"Successfully closed.\")\n",
    "else:\n",
    "    print(\"File connection is still open.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the contents of a particular index array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pp.pprint(idx_array[0:2, 0:5,0:5])\n",
    "print(\"dtype:\", idx_array.dtype)\n",
    "print(\"unique:\", np.unique(idx_array))\n",
    "print(\"sum =\", np.sum(idx_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that we can just sum these indicators to check how many voxels there are in each ROI.\n",
    "\n",
    "### Exercises (C):\n",
    "\n",
    "0. Use `np.nonzero` to get the actual indices that are \"hot\".\n",
    "\n",
    "0. How many voxels are there in the ROI called V4 in the left hemisphere?\n",
    "\n",
    "0. How many voxels are in V4 in total? (both left and right hemispheres)\n",
    "\n",
    "0. Is this number the same across all subjects?\n",
    "\n",
    "0. Try summing the contents of all the index matrices. If the sum is precisely 73728 (the number of voxels), then it is reasonable to expect that each voxel appears in one and only one ROI. If the sum is less than this number, some voxels necessarily do not belong to any ROIs. If the sum is greater that this number, some voxels must belong to more than one ROI. Which of these is the case?\n",
    "\n",
    "0. Make a bar graph (see `help(plt.bar)`) for each region. Which region is largest? Smallest? Is there a difference between left/right hemispheres, or are they symmetric?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea what the index does, let's examine the actual response values for a few voxels over the first couple minutes of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open file connection.\n",
    "with tables.open_file(\"data/vim-2/VoxelResponses_subject1.mat\", mode=\"r\") as f:\n",
    "    \n",
    "    idx_object_myroi = f.get_node(where=\"/roi/\", name=\"v4lh\")\n",
    "    idx_array_myroi = idx_object_myroi.read()\n",
    "    \n",
    "    indices = np.nonzero(idx_array_myroi.flatten())[0] # returns a tuple; extract the array.\n",
    "    response_myroi = np.take(a=response_array, axis=0, indices=indices)\n",
    "    print(\"shape:\", response_myroi.shape)\n",
    "    print(\"sum of index:\", np.sum(idx_array_myroi))\n",
    "    \n",
    "num_secs = 120\n",
    "time_idx = np.arange(0, num_secs, 1)\n",
    "\n",
    "myfig = plt.figure(figsize=(10,8))\n",
    "\n",
    "myfig.add_subplot(4,1,1)\n",
    "plt.title(\"BOLD signal response from a handful of voxels\")\n",
    "val = response_myroi[0,time_idx]\n",
    "plt.plot(time_idx, val)\n",
    "print(\"dtype:\", val.dtype)\n",
    "print(\"num of nans:\", np.sum(np.isnan(val)))\n",
    "\n",
    "myfig.add_subplot(4,1,2)\n",
    "val = response_myroi[1,time_idx]\n",
    "plt.plot(time_idx, val)\n",
    "print(\"dtype:\", val.dtype)\n",
    "print(\"num of nans:\", np.sum(np.isnan(val)))\n",
    "\n",
    "myfig.add_subplot(4,1,3)\n",
    "val = response_myroi[2,time_idx]\n",
    "plt.plot(time_idx, val)\n",
    "print(\"dtype:\", val.dtype)\n",
    "print(\"num of nans:\", np.sum(np.isnan(val)))\n",
    "\n",
    "myfig.add_subplot(4,1,4)\n",
    "val = response_myroi[3,time_idx]\n",
    "plt.plot(time_idx, val)\n",
    "print(\"dtype:\", val.dtype)\n",
    "print(\"num of nans:\", np.sum(np.isnan(val)))\n",
    "\n",
    "plt.xlabel(\"Elapsed time (s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The responses are rather erratic, but at least appear to be correctly read. The main issue that we run into here is the existence of *missing values*.\n",
    "\n",
    "To find the bad voxels is not difficult, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_voxels = response_array.shape[0]\n",
    "cleanidx = np.zeros((num_voxels,), dtype=np.uint32)\n",
    "\n",
    "# Loop over the response array.\n",
    "for v in range(num_voxels):\n",
    "    \n",
    "    if np.sum(np.isnan(response_array[v,:])) == 0:\n",
    "        cleanidx[v] = 1 # on if the voxel is clean, otherwise off.\n",
    "\n",
    "num_good = np.sum(cleanidx)\n",
    "num_bad = np.int(num_voxels-num_good)\n",
    "print(\"total good =\", num_good, \";\", format(100*num_good/num_voxels, \".2f\"), \"percent.\")\n",
    "print(\"total bad =\", num_bad, \";\", format(100*num_bad/num_voxels, \".2f\"), \"percent.\")\n",
    "\n",
    "\n",
    "with open(\"data/vim-2/cleanidx_tr.dat\", mode=\"bw\") as fbin:\n",
    "    cleanidx.tofile(fbin)\n",
    "    print(\"Saved to file.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear that most of the voxels have no missing values.\n",
    "\n",
    "### Exercises (D):\n",
    "\n",
    "0. Compare the first 120 seconds of the first four voxels of `v1lh`, with those of `v4lh`. Note that the former ROI has voxels with missing values; the usual placeholder for these is `nan`.\n",
    "\n",
    "0. How many voxels have missing values on the training data? Make a histogram of with counts of \"bad\" voxels for each ROI. Are some regions more prone to missing values than others? Is there much difference between the left and right hemispheres of the same ROIs?\n",
    "\n",
    "0. Store the \"clean indices\" for the training data (call it `cleanidx_tr`) in addition to that for the testing data. Since we will in the end only want to train a model for voxels with both training and testing observations available, compute the intersection of `cleanidx_tr` and `cleanidx_te`, and save this as simply `cleanidx`.\n",
    "\n",
    "___\n",
    "\n",
    "As with the input data, let us save the response data in a form that is quickly readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"data/vim-2/y_te.dat\"\n",
    "dtype = response_array.dtype\n",
    "shape = response_array.shape\n",
    "with open(fname, mode=\"bw\") as fbin:\n",
    "    response_array.tofile(fbin)\n",
    "    print(\"Saved to file.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the data is as we expect after saving it to file, let's read it, re-shape it, and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wipe the response array.\n",
    "response_array = np.zeros(response_array.size, dtype=response_array.dtype).reshape(response_array.shape)\n",
    "\n",
    "# Load up the stored array.\n",
    "with open(fname, mode=\"br\") as fbin:\n",
    "    print(\"Reading...\", end=\" \")\n",
    "    response_array = np.fromfile(file=fbin, dtype=dtype).reshape(shape)\n",
    "    print(\"OK.\")\n",
    "\n",
    "# Do the exact same visualization, and see if things change.    \n",
    "num_secs = 120\n",
    "time_idx = np.arange(0, num_secs, 1)\n",
    "\n",
    "myfig = plt.figure(figsize=(10,8))\n",
    "\n",
    "myfig.add_subplot(4,1,1)\n",
    "plt.title(\"BOLD signal response from a handful of voxels\")\n",
    "val = response_myroi[0,time_idx]\n",
    "plt.plot(time_idx, val)\n",
    "print(\"dtype:\", val.dtype)\n",
    "print(\"num of nans:\", np.sum(np.isnan(val)))\n",
    "\n",
    "myfig.add_subplot(4,1,2)\n",
    "val = response_myroi[1,time_idx]\n",
    "plt.plot(time_idx, val)\n",
    "print(\"dtype:\", val.dtype)\n",
    "print(\"num of nans:\", np.sum(np.isnan(val)))\n",
    "\n",
    "myfig.add_subplot(4,1,3)\n",
    "val = response_myroi[2,time_idx]\n",
    "plt.plot(time_idx, val)\n",
    "print(\"dtype:\", val.dtype)\n",
    "print(\"num of nans:\", np.sum(np.isnan(val)))\n",
    "\n",
    "myfig.add_subplot(4,1,4)\n",
    "val = response_myroi[3,time_idx]\n",
    "plt.plot(time_idx, val)\n",
    "print(\"dtype:\", val.dtype)\n",
    "print(\"num of nans:\", np.sum(np.isnan(val)))\n",
    "\n",
    "plt.xlabel(\"Elapsed time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comparing the figure just produced with the one based on the original data, they clearly look identical, assuming everything was done correctly.\n",
    "\n",
    "### Exercises (E):\n",
    "\n",
    "0. Prepare `y_tr` using `rt` in a manner completely analogous to `y_te` using `rv` above.\n",
    "\n",
    "0. As practice with basic plotting tools, create a figure composed of 8 plots, in a 4 x 2 (4 rows, two columns) format, with the following content. Choose four \"clean\" voxels from the training data, in the ROI called V1, where both left and right hemispheres have no missing data. These voxels correspond to the four rows. The columns correspond to the left and right hemispheres respectively (for a total of 8 voxels, one voxel per plot). The content of each plot is simply the first 60 seconds of measured activity.\n",
    "\n",
    "0. Finally, the clean indices and response computations above have only been computed for a single subject. Carry out the above procedures for each separate subject, perhaps appending `sub1`, `sub2`, `sub3` etc. to the filenames. For example, instead of a single non-specific `y_tr.dat`, we should have `y_tr_sub1.dat`, `y_tr_sub2.dat`, `y_tr_sub3.dat`, and so forth for the `X_tr`, `X_te`, `y_te` as well. Same deal for the clean indices.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"resp_pproc\"></a>\n",
    "## Regarding pre-processing of responses\n",
    "\n",
    "Now, raw BOLD signals typically require some pre-processing. Fortunately, that work has been essentially done for us. To confirm this, let's check the documentation again. Quoting from the *vim-2* dataset description file,\n",
    "\n",
    "> *\"The functional data were collected for three subjects, in three sessions over three separate days for each subject (please see the Nishimoto et al. 2011 for scanning and preprocessing parameters). Peak BOLD responses to each timepoint in the stimuli were estimated from the preprocessed data.\"*\n",
    "\n",
    "This tells us that we do not need to mess around with the response values. For reference, to unpack the pre-processing that was used to estimate peak BOLD responses, they provide a lucid description in the appendix of the cited paper (*Supplemental Experimental Procedures -- Data Preprocessing*):\n",
    "\n",
    "> *\"BOLD signals were preprocessed as described in earlier publications. Briefly, motion compensation was performed using SPM '99, and supplemented by additional custom algorithms. For each 10 minute run and each individual voxel, drift in BOLD signals was first removed by fitting a third-degree polynomial, and signals were then normalized to mean 0.0 and standard deviation 1.0.\"*\n",
    "\n",
    "To be clear, the peak response estimates are *based* on data that was pre-processed as above. It is *not* the case that the final per-block estimates themselves have been normalized in this fashion. That said, it is easily confirmed that the sample mean and standard deviation of each 10-minute block (of the training data) is approximately centered and with unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load up the training data.\n",
    "fname = \"data/vim-2/y_tr.dat\"\n",
    "with open(fname, mode=\"br\") as fbin:\n",
    "    print(\"Reading...\", end=\" \")\n",
    "    response_array = np.fromfile(file=fbin, dtype=np.float32).reshape((73728, 7200))\n",
    "    print(\"OK.\")\n",
    "    \n",
    "fname = \"data/vim-2/cleanidx_tr.dat\"\n",
    "with open(fname, mode=\"br\") as fbin:\n",
    "    print(\"Reading...\", end=\" \")\n",
    "    cleanidx_tr = np.fromfile(file=fbin, dtype=np.uint32)\n",
    "    print(\"OK.\")\n",
    "    \n",
    "response_array_clean = np.take(a=response_array, axis=0, indices=np.nonzero(cleanidx_tr)[0])\n",
    "\n",
    "voxel_idx = 0 # the (clean) voxel idx to check\n",
    "tmpspan = 600 # 10 minute \"blocks\"; units are seconds.\n",
    "for i in range(response_array_clean.shape[1]//tmpspan):\n",
    "    tmpi = i\n",
    "    tmpidx = np.arange((tmpi*tmpspan), (tmpi+1)*tmpspan)\n",
    "    response_tocheck = response_array_clean[voxel_idx,tmpidx]\n",
    "    print(\"Block num =\", i, \", mean =\", np.mean(response_tocheck), \", std =\", np.std(response_tocheck))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of lesson: paste any routines to be re-used in the `scripts/vim-2.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References:\n",
    "\n",
    " - Nishimoto, Shinji, et al. \"Reconstructing visual experiences from brain activity evoked by natural movies.\" Current Biology 21.19 (2011): 1641-1646.\n",
    " - Description of dataset vim-2 (visual imaging 2), at CRCNS - Collaborative Research in Computational Neuroscience. https://crcns.org/data-sets/vc/vim-2/about-vim-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
